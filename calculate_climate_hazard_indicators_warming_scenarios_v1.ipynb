{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49716883",
   "metadata": {},
   "source": [
    "This script takes these as inputs\n",
    "\n",
    "* City latlon file\n",
    "* Best-models and calibration-function file\n",
    "* Scenario-years file\n",
    "* Historical observed, historical modeled, and future modeled meteorological data stored in Google Earth Engine\n",
    "\n",
    "...and estimates for each city a set of future indicator values based on 14 climate hazards. The indicators are expected value and probabilities of exceeding given hazard thresholds, both estimated for given global warming scenario years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abdba210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/cloud-platform%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=TGkJzufofzySGVWF8HR8CfCa4cklrDTKBXOUCzOh34o&tc=dNjQHjNEqMdEp2eSfTJvq1UaUths_UNX27kStI7QbwA&cc=OFeOyhRBi-8OI6Ro5p_3sNQAtKLmE5jl2mvYa5DP9xo>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/cloud-platform%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=TGkJzufofzySGVWF8HR8CfCa4cklrDTKBXOUCzOh34o&tc=dNjQHjNEqMdEp2eSfTJvq1UaUths_UNX27kStI7QbwA&cc=OFeOyhRBi-8OI6Ro5p_3sNQAtKLmE5jl2mvYa5DP9xo</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you should paste in the box below.</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter verification code: 4/1AdLIrYde4WAwSB4dzwVMDzznxZDLh6GGL0ru-GhZfxXRWrRmSNhKFVPGPf8\n",
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import math\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import scipy\n",
    "\n",
    "import datetime, calendar\n",
    "import spei\n",
    "\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "254785d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIST_START = 1980\n",
    "HIST_END = 2014\n",
    "\n",
    "PERCENTILE_STARTYEAR = 1980\n",
    "PERCENTILE_ENDYEAR = 2019\n",
    "\n",
    "NUM_BEST_MODELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66e61532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit conversions and variable names for NEX-GDDP-CMIP6 and ERA5\n",
    "\n",
    "VARIABLES = {\n",
    "    'tas': {\n",
    "        'era_varname': 'mean_2m_air_temperature',\n",
    "        'nex_transform': lambda x: x - 273.5,\n",
    "        'era_transform': lambda x: x - 273.5\n",
    "    },\n",
    "    'tasmax': {\n",
    "        'era_varname': 'maximum_2m_air_temperature',\n",
    "        'nex_transform': lambda x: x - 273.5,\n",
    "        'era_transform': lambda x: x - 273.5\n",
    "    },\n",
    "    'tasmin': {\n",
    "        'era_varname': 'minimum_2m_air_temperature',\n",
    "        'nex_transform': lambda x: x - 273.5,\n",
    "        'era_transform': lambda x: x - 273.5\n",
    "    },\n",
    "    'pr': {\n",
    "        'era_varname': 'total_precipitation',\n",
    "        'nex_transform': lambda x: x * 86400,\n",
    "        'era_transform': lambda x: x * 1000\n",
    "    },\n",
    "    'hurs': {\n",
    "        'era_varname': None,\n",
    "       'nex_transform': lambda x: x,\n",
    "        'era_transform': lambda x: x\n",
    "    },\n",
    "    'maxwetbulb': {\n",
    "        'era_varname': None,\n",
    "       'nex_transform': lambda x: x,\n",
    "        'era_transform': lambda x: x\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af292bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = ['MIROC-ES2L', 'UKESM1-0-LL', 'EC-Earth3-Veg-LR', 'GFDL-ESM4', 'INM-CM5-0', 'MRI-ESM2-0', 'FGOALS-g3', 'IPSL-CM6A-LR', 'CanESM5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f97dfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read city-latlon file. File is header-less CSV. Columns are:\n",
    "# idnum, UNUSED, latitude, longidude, UNUSED, UNUSED, UNUSED, UNUSED\n",
    "\n",
    "CITYLATLON = {}\n",
    "with open('ghsl_500k.csv', 'r') as ifile:\n",
    "    for line in ifile.readlines():\n",
    "        items = [i.strip() for i in line.split(',')]\n",
    "        CITYLATLON['city_{0}'.format(items[0])] = (float(items[2]), float(items[3]), int(items[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "001545c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calendardate_percentiles(nex_varname, q, latlon, sh_hem=False):\n",
    "    # Percentiles based on given calendar date\n",
    "    hist_start = PERCENTILE_STARTYEAR\n",
    "    hist_end = PERCENTILE_ENDYEAR\n",
    "    allyears = []\n",
    "    for year in range(hist_start, hist_end):\n",
    "        allyears.append(get_observed_gee(nex_varname, latlon, start_year=year, end_year=year, southern_hem=False))\n",
    "    if not sh_hem:\n",
    "        return np.percentile(np.vstack(allyears), q, axis=0)\n",
    "    else:\n",
    "        res = np.percentile(np.vstack(allyears), q, axis=0)\n",
    "        return np.concatenate([res[152:], res[:152]])\n",
    "\n",
    "def wholeyear_percentile(nex_varname, q, latlon):\n",
    "    # percentiles based not on one calendar date, but on all dates\n",
    "    if not nex_varname == 'ari':\n",
    "        hist_start = PERCENTILE_STARTYEAR\n",
    "        hist_end = PERCENTILE_ENDYEAR\n",
    "        allyears = []\n",
    "        for year in range(hist_start, hist_end):\n",
    "            allyears.append(get_observed_gee(nex_varname, latlon, start_year=year, end_year=year, southern_hem=False))\n",
    "        return np.percentile(np.concatenate(allyears).flatten(), q)\n",
    "    else:\n",
    "        hist_start = PERCENTILE_STARTYEAR\n",
    "        hist_end = PERCENTILE_ENDYEAR\n",
    "        allyears = []\n",
    "        for year in range(hist_start, hist_end):\n",
    "            allyears.append(get_observed_gee('pr', latlon, start_year=year, end_year=year, southern_hem=False))\n",
    "        ari_data = ari(np.concatenate(allyears).flatten())\n",
    "        return np.percentile(ari_data, 95)\n",
    "\n",
    "def yearextreme_percentile(nex_varname, q, latlon, wantmax):\n",
    "    # Percentiles \n",
    "    hist_start = PERCENTILE_STARTYEAR\n",
    "    hist_end = PERCENTILE_ENDYEAR\n",
    "    allyears = get_observed_gee(nex_varname, latlon, hist_start, hist_end, southern_hem=False)\n",
    "    return np.percentile(allyears, q)\n",
    "\n",
    "def d2j(datestring):\n",
    "    # Date to Julian date\n",
    "    d = datetime.date.fromisoformat(datestring)\n",
    "    jday = d.timetuple().tm_yday\n",
    "    if calendar.isleap(d.year) and jday > 59:\n",
    "        jday -= 1\n",
    "    return jday\n",
    "\n",
    "def removeLeapDays(arr, start_year, end_year, southern_hem):\n",
    "    indices_to_remove = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        if calendar.isleap(year):\n",
    "            indices_to_remove.append(((year-start_year) * 365) + [0,183][int(southern_hem)] + len(indices_to_remove) + 59)\n",
    "    return np.delete(arr, indices_to_remove)\n",
    "\n",
    "def count_runs(tf_array, min_runsize):\n",
    "    # Return number of consecutive runs or at least minimum run length\n",
    "    # Accepts true-false array with one row per year, returns array of counts -- one count per year\n",
    "    falses = np.zeros(tf_array.shape[0]).reshape((tf_array.shape[0],1))\n",
    "    extended_a = np.concatenate([[0], tf_array, [0]])\n",
    "    df = np.diff(extended_a)\n",
    "    starts = np.nonzero(df == 1)[0]\n",
    "    ends = np.nonzero(df == -1)[0]\n",
    "    count = 0\n",
    "    for idx in range(starts.size):\n",
    "        if ends[idx] - starts[idx] >= min_runsize:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def longest_run(tf_array):\n",
    "    # Return nduration of longest consecutive run\n",
    "    # Accepts true-false array with one row per year, returns array of longest durations -- one duration per year\n",
    "    if np.sum(tf_array) == 0:\n",
    "        return 0\n",
    "    falses = np.zeros(tf_array.shape[0]).reshape((tf_array.shape[0],1))\n",
    "    extended_a = np.concatenate([[0], tf_array, [0]])\n",
    "    df = np.diff(extended_a)\n",
    "    starts = np.nonzero(df == 1)[0]\n",
    "    ends = np.nonzero(df == -1)[0]\n",
    "    durations = ends - starts\n",
    "    return max(durations)\n",
    "    \n",
    "def quarters(d, start_year, end_year, southern_hem=False):\n",
    "    #Takes multi-year array and returns data reorganized into seasonal quarters\n",
    "    q2 = []  # 60-151\n",
    "    q3 = []  # 152-243\n",
    "    q4 = []  # 244-334\n",
    "    q1 = []  # 335-59\n",
    "    if not southern_hem:\n",
    "        jan1_idx = 365\n",
    "        for year in range(start_year, end_year):\n",
    "            tmp = np.concatenate((d[jan1_idx - 365 : jan1_idx - 365 + 60], d[jan1_idx + 335 : jan1_idx + 365]), axis=0)\n",
    "            q1.append(tmp)\n",
    "            q2.append(d[jan1_idx + 60 : jan1_idx + 152])\n",
    "            q3.append(d[jan1_idx + 152 : jan1_idx + 244])\n",
    "            q4.append(d[jan1_idx + 244 : jan1_idx + 335])\n",
    "\n",
    "            jan1_idx += 365 + [0, 0][int(False and calendar.isleap(year))]\n",
    "        mam_res = np.vstack(q2)\n",
    "        jja_res = np.vstack(q3)\n",
    "        son_res = np.vstack(q4)\n",
    "        djf_res = np.vstack(q1)\n",
    "    else:\n",
    "        jul1_idx = 365\n",
    "        for year in range(start_year, end_year):\n",
    "            tmp = np.concatenate((d[jul1_idx - 365 : jul1_idx - 365 + 60], d[jul1_idx + 335 : jul1_idx + 365]), axis=0)\n",
    "            q3.append(tmp)\n",
    "            q4.append(d[jul1_idx + 60 : jul1_idx + 152])\n",
    "            q1.append(d[jul1_idx + 152 : jul1_idx + 244])\n",
    "            q2.append(d[jul1_idx + 244 : jul1_idx + 335])\n",
    "\n",
    "            jul1_idx += 365 + [0, 0][int(False and calendar.isleap(year))]\n",
    "        mam_res = np.vstack(q4)\n",
    "        jja_res = np.vstack(q1)\n",
    "        son_res = np.vstack(q2)\n",
    "        djf_res = np.vstack(q3)\n",
    "    return mam_res, jja_res, son_res, djf_res\n",
    "\n",
    "def calibrate_component(uncalibrated_data, calibration_fxn):\n",
    "    # Applies calib fxn to one data array -- generally used for one quarter's worth of data\n",
    "    N = len(uncalibrated_data)\n",
    "    unsorted_uncalib = [(i, idx) for idx, i in enumerate(uncalibrated_data)]\n",
    "    sorted_uncalib = sorted(unsorted_uncalib)\n",
    "    result = [0] * N\n",
    "    for j in range(N):\n",
    "        X_j = j / (N + 1)\n",
    "        Y_jprime = calibration_fxn[math.floor(X_j * len(calibration_fxn))]\n",
    "        jprime = math.floor(Y_jprime * (N + 1))\n",
    "        result[sorted_uncalib[j][1]] = sorted_uncalib[min(len(sorted_uncalib)-1, jprime)][0]\n",
    "    return result\n",
    "\n",
    "def calibrate(uncalibrated_data, calibration_fxn):\n",
    "    # Applies calibrate_component() to full all-years array, applying quarter-specific calib fxns separately\n",
    "    mam = []\n",
    "    jja = []\n",
    "    son = []\n",
    "    djf = []\n",
    "    mam_idx = []\n",
    "    jja_idx = []\n",
    "    son_idx = []\n",
    "    djf_idx = []\n",
    "    for idx, i in enumerate(uncalibrated_data):\n",
    "        if idx % 365 >= 60 and idx % 365 < 152:\n",
    "            mam.append(uncalibrated_data[idx])\n",
    "            mam_idx.append(idx)\n",
    "        elif idx % 365 >= 152 and idx % 365 < 244:\n",
    "            jja.append(uncalibrated_data[idx])\n",
    "            jja_idx.append(idx)\n",
    "        elif idx % 365 >= 244 and idx % 365 < 335:\n",
    "            son.append(uncalibrated_data[idx])\n",
    "            son_idx.append(idx)\n",
    "        else:\n",
    "            djf.append(uncalibrated_data[idx])\n",
    "            djf_idx.append(idx)\n",
    "    \n",
    "    mam_calib = calibrate_component(np.array(mam), calibration_fxn[0])\n",
    "    jja_calib = calibrate_component(np.array(jja), calibration_fxn[1])\n",
    "    son_calib = calibrate_component(np.array(son), calibration_fxn[2])\n",
    "    djf_calib = calibrate_component(np.array(djf), calibration_fxn[3])\n",
    "    \n",
    "    result = [0] * len(uncalibrated_data)\n",
    "    for i in range(len(mam_idx)):\n",
    "        result[mam_idx[i]] = mam_calib[i]\n",
    "    for i in range(len(jja_idx)):\n",
    "        result[jja_idx[i]] = jja_calib[i]\n",
    "    for i in range(len(son_idx)):\n",
    "        result[son_idx[i]] = son_calib[i]\n",
    "    for i in range(len(djf_idx)):\n",
    "        result[djf_idx[i]] = djf_calib[i]\n",
    "\n",
    "    return np.array(result)\n",
    "\n",
    "def ari(yeardata):\n",
    "    ARI_WEIGHTS = np.array([\n",
    "        0.013499274414000246,\n",
    "        0.01837401239683367,\n",
    "        0.026458577851440485,\n",
    "        0.041341527892875755,\n",
    "        0.07349604958733467,\n",
    "        0.16536611157150302,\n",
    "        0.6614644462860121\n",
    "    ])\n",
    "    def ari_7day(sevendayrain):\n",
    "        return np.dot(sevendayrain, ARI_WEIGHTS)\n",
    "    \n",
    "    res = []\n",
    "    for start_idx in range(yeardata.size-7):\n",
    "        res.append(ari_7day(yeardata[start_idx:start_idx+7]))\n",
    "    return res\n",
    "\n",
    "def get_ari95(latlon):\n",
    "#    ARI95 = ee.Image('users/tedwongwri/dataportal/landslide/ARI95')\n",
    "#    geom = ee.Geometry.Point((latlon[1], latlon[0]))\n",
    "#    res = ARI95.reduceRegion(ee.Reducer.mean(), geom, 10, 'epsg:4326').getInfo()['b1']\n",
    "#    return res\n",
    "    return wholeyear_percentile('ari', 95, latlon)\n",
    "        \n",
    "\n",
    "def wetbulbtemp(T, RH):\n",
    "# JA Knox et al. 2017. Two simple and accurate approximations for wet-bulb\n",
    "# temperature in moist conditions, with forecasting applications. Bull. Am.\n",
    "# Meteorol. Soc. 98(9): 1897-1906. doi:10.1175/BAMS-D-16-0246.1\n",
    "    T = T.astype(np.float64)\n",
    "    rh_percent = RH.astype(np.float64)\n",
    "    return T * np.arctan(0.151977 * np.sqrt(rh_percent + 8.313659)) + np.arctan(T + rh_percent) - np.arctan(rh_percent - 1.676331) + ((0.00391838 * ((rh_percent)**(3/2))) * np.arctan(0.023101 * rh_percent)) - 4.686035        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05aa86a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Hazard defines two indicator fxns and beqeaths to a number of hazard-specific subclasses. The indicator fxns\n",
    "# get_expectedval() and get_exceedanceprob() accept an array of meteorological data and return statistics of the\n",
    "# estimated predicted expected value of the hazard magnitude, or the probability that the hazard magnitude exceeds\n",
    "# a given extreme value, in the years represented in the data array. The indicator statistics are currently mean estimate\n",
    "# and standard deviation of the estimate.\n",
    "#\n",
    "# Hazard subclasses each define a val_dist() fxn, which accepts a data array (one row per meteorological variable,\n",
    "# one column for each day in the multi-year interval of interest) and returns a dict whose keys are hazard magnitudes\n",
    "# and whose values are how many times (i.e., years) in the multi-year interval that value is observed in the data array.\n",
    "# The dict is a frequency distribution. Both get_expectedval() and get_exceedanceprob() call on val_dist().\n",
    "#\n",
    "# Each hazard subclass is instantiated with the arguments necessary to calculate hazard magnitude. Some hazards\n",
    "# require threshold values, some require range endmembers, and some require the minimum number of consecutive\n",
    "# days to count as a run of days. All hazards also require argument extremeval, which is the threshold magnitude used\n",
    "# to calculate threshold-exceedance probability.\n",
    "\n",
    "class Hazard:\n",
    "    def get_percentile(self, latlon, q):\n",
    "        southern_hem = int(latlon[0] < 0)\n",
    "        era_data = {}\n",
    "        scenario_years = {}\n",
    "        varnames = self.varname.split('+')\n",
    "        for varname in varnames:\n",
    "            era_data[varname] = get_observed_gee(varname, latlon, PERCENTILE_STARTYEAR, PERCENTILE_ENDYEAR, southern_hem)\n",
    "        countdist = self.val_dist([ed for ed in era_data])\n",
    "        return np.percentile(countdist, q)\n",
    "    \n",
    "    def get_expectedval(self, latlon, datasets, calib_fxns):\n",
    "        # Take uncalibrated data, calibrate it, apply val_dist() to calibrated data, use resulting freq dist\n",
    "        # to parameterize Dirichlet prior, take resulting vector to parameterize multinomial distribution,\n",
    "        # sample from that multinomial to generate predictive distribution of freq distributions, and return statistics\n",
    "        # (mean and stdev but it could be anything) of the predictive distribution.\n",
    "        \n",
    "        southern_hem = int(latlon[0] < 0)\n",
    "        fut_mod = {}\n",
    "        scenario_years = {}\n",
    "        varnames = list(datasets.keys())\n",
    "        for varname in varnames:\n",
    "            for model in calib_fxns[self.varname].keys():\n",
    "                ds = datasets[varname][model][2]\n",
    "                scenario_years[model] = (datasets[varname][model][0], datasets[varname][model][1])\n",
    "                fut_mod[(varname, model)] = ds\n",
    "        best_models = []\n",
    "        for idx in range(NUM_BEST_MODELS):\n",
    "            modelplus = '+'.join([list(calib_fxns[self.varname].keys())[idx] for varname in varnames])\n",
    "            best_models.append(modelplus)\n",
    "\n",
    "        para_res = {}\n",
    "        for modelplus in best_models:\n",
    "            start_year = scenario_years[modelplus.split('+')[0]][0]\n",
    "            end_year = scenario_years[modelplus.split('+')[0]][1]\n",
    "            numbins = end_year - start_year + 1\n",
    "            calib_data = []\n",
    "            for idx, varname in enumerate(varnames):\n",
    "                model = modelplus.split('+')[idx]\n",
    "                calib_data.append(np.array(calibrate(fut_mod[(varname, model)], calib_fxns[self.varname][model])))\n",
    "            countdist = self.val_dist([cd[[0,152][int(not southern_hem)]:[len(cd),-213][int(not southern_hem)]] for cd in calib_data])\n",
    "            if countdist is None:\n",
    "                para_res[modelplus] = None\n",
    "            else:\n",
    "                observed_vals = np.array(list(countdist.keys()))\n",
    "                cdist = {}\n",
    "                minval = observed_vals[0]\n",
    "                maxval = observed_vals[-1]\n",
    "                D = (maxval - minval) / (numbins - 1)\n",
    "                for i in range(numbins):\n",
    "                    centerval = minval + (i * D)\n",
    "                    cdist[centerval] = 0\n",
    "                for count in countdist:\n",
    "                    for centerval in cdist:\n",
    "                        if count >= centerval - (D/2) and count < centerval + (D/2):\n",
    "                            cdist[centerval] += 1\n",
    "                alpha = np.array(list(cdist.values())) + (1/numbins)\n",
    "                res = []\n",
    "                for i in range(10000):\n",
    "                    dirich_samp = np.random.dirichlet(alpha, 1)\n",
    "                    mult_samp = np.random.multinomial(end_year - start_year + 1, dirich_samp[0], 1)[0]\n",
    "                    res.append(sum([list(cdist.keys())[j] * mult_samp[j] for j in range(len(list(cdist.keys())))]) / (end_year - start_year + 1))\n",
    "                res = np.array(res)\n",
    "                para_res[modelplus] = res\n",
    "        result = {}\n",
    "        for modelplus in best_models:\n",
    "            if para_res[modelplus] is None:\n",
    "                result[modelplus] = [-9999, -9999, -9999]\n",
    "            else:\n",
    "                result[modelplus] = [np.mean(para_res[modelplus]), np.std(para_res[modelplus]), -9999]  # -9999 is dummy to preserve format from earier version\n",
    "        return result\n",
    "    \n",
    "    def get_exceedanceprob(self, latlon, datasets, calib_fxns):\n",
    "        # Take uncalibrated data, calibrate it, apply val_dist() to calibrated data, use resulting freq dist\n",
    "        # to generate analytically the mean and stdev of beta-binomial or gamma-poisson distribution of\n",
    "        # threshold-exceedance probabilities. Return statistics (mean and stdev but it could be anything) of\n",
    "        # the predictive distribution.\n",
    "        \n",
    "        def floorceiling_prob(p):\n",
    "            # Ensure that probabilities are on [0, 1]\n",
    "            return max(0, min(1, p))\n",
    "        \n",
    "        southern_hem = int(latlon[0] < 0)\n",
    "        fut_mod = {}\n",
    "        scenario_years = {}\n",
    "        varnames = list(datasets.keys())\n",
    "        for varname in varnames:\n",
    "            for model in calib_fxns[self.varname].keys():\n",
    "                ds = datasets[varname][model][2]\n",
    "                scenario_years[model] = (datasets[varname][model][0], datasets[varname][model][1])\n",
    "                fut_mod[(varname, model)] = ds\n",
    "        best_models = []\n",
    "        for idx in range(NUM_BEST_MODELS):\n",
    "            modelplus = '+'.join([list(calib_fxns[self.varname].keys())[idx] for varname in varnames])\n",
    "            best_models.append(modelplus)\n",
    "\n",
    "        prob_res = {}\n",
    "        for modelplus in best_models:\n",
    "            prob_res[modelplus] = {}\n",
    "            start_year = scenario_years[modelplus.split('+')[0]][0]\n",
    "            end_year = scenario_years[modelplus.split('+')[0]][1]\n",
    "            calib_data = []\n",
    "            for idx, varname in enumerate(varnames):\n",
    "                model = modelplus.split('+')[idx]\n",
    "                calib_data.append(np.array(calibrate(fut_mod[(varname, model)], calib_fxns[self.varname][model])))\n",
    "            countdist = self.val_dist([cd[[0,152][int(not southern_hem)]:[len(cd),-213][int(not southern_hem)]] for cd in calib_data])\n",
    "            for exval in self.extremeval:\n",
    "                if countdist is None:\n",
    "                    prob_res[modelplus][exval] = [-9999, -9999, -9999]\n",
    "                else:\n",
    "                    count = sum([countdist[val] * int((val >= exval and self.exceed_is_gte) or (val <= exval and not self.exceed_is_gte)) for val in countdist])\n",
    "                    N = end_year - start_year + 1\n",
    "                    if self.probmodel == 'Poisson':\n",
    "                        meanprob = (count + 0.5) / N\n",
    "                        stdprob = np.sqrt((2 * count) + 1) / N\n",
    "                    else: \n",
    "                        meanprob = ((count + 0.5) * N / (N + 1)) / N\n",
    "                        stdprob = (np.sqrt((N * (count + 0.5) * (N - count + 0.5) * ((2 * N) + 1)) / ((N + 2) * (N + 1) * (N + 1)))) / N\n",
    "                    prob_res[modelplus][exval] = [floorceiling_prob(meanprob), stdprob, -9999]  # -9999 is dummy to preserve format from earier version\n",
    "       \n",
    "        return prob_res\n",
    "\n",
    "\n",
    "\n",
    "class Tempwave_count(Hazard):\n",
    "    # Heatwaves and coldwaves\n",
    "    # Number per year\n",
    "    # tf_gte is True for heatwaves, False for coldwaves\n",
    "    def __init__(self, hazname, varname, min_duration, threshold, tf_gte, extremeval):\n",
    "        if type(threshold) == np.ndarray and threshold.size % 365 != 0:\n",
    "            raise Exception('Comparison array length is not an integer multiple of 365')\n",
    "        self.hazname = hazname\n",
    "        self.varname = varname\n",
    "        self.tf_gte = tf_gte\n",
    "        self.min_duration = min_duration\n",
    "        self.threshold = threshold  # May be scalar or 365-long array\n",
    "        self.probmodel = 'Poisson'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "        \n",
    "    def tf_array(self, datalist):\n",
    "        data = datalist[0]\n",
    "        if self.tf_gte:\n",
    "            return data >= self.threshold\n",
    "        else:\n",
    "            return data <= self.threshold\n",
    "\n",
    "    def val_dist(self, datalist):\n",
    "        tfarray = self.tf_array(datalist)\n",
    "        tfarray = tfarray.reshape(tfarray.size//365, 365)\n",
    "        vals = np.apply_along_axis(count_runs, 1, tfarray, self.min_duration)\n",
    "        \n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "    \n",
    "class Tempwave_duration(Hazard):\n",
    "    # Heatwaves and coldwaves\n",
    "    # Duration of longest in a year\n",
    "    # tf_gte is True for heatwaves, False for coldwaves\n",
    "    def __init__(self, hazname, varname, threshold, tf_gte, extremeval):\n",
    "        if type(threshold) == np.ndarray and threshold.size % 365 != 0:\n",
    "            raise Exception('Comparison array length is not an integer multiple of 365')\n",
    "        self.hazname = hazname\n",
    "        self.varname = varname\n",
    "        self.tf_gte = tf_gte\n",
    "        self.threshold = threshold  # May be scalar or 365-long array\n",
    "        self.probmodel = 'Poisson'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "        \n",
    "    def tf_array(self, datalist):\n",
    "        data = datalist[0]\n",
    "        if self.tf_gte:\n",
    "            return data >= self.threshold\n",
    "        else:\n",
    "            return data <= self.threshold\n",
    "\n",
    "    def val_dist(self, datalist):\n",
    "        tfarray = self.tf_array(datalist)\n",
    "        tfarray = tfarray.reshape(tfarray.size//365, 365)\n",
    "        vals = np.apply_along_axis(longest_run, 1, tfarray)\n",
    "        \n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "\n",
    "class Tempwave_highlow_count(Hazard):\n",
    "    # Consecutive run of days with temp within a range. Number of these runs in a year.\n",
    "    def __init__(self, hazname, hightemp, lowtemp, min_duration, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = 'tasmax+tasmin'\n",
    "        self.min_duration = min_duration\n",
    "        self.hightemp = hightemp\n",
    "        self.lowtemp = lowtemp\n",
    "        self.probmodel = 'Poisson'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "        \n",
    "    def tf_array(self, datalist):\n",
    "        data_tx = datalist[0]\n",
    "        data_tn = datalist[1]\n",
    "        if type(self.hightemp) in (float, int, np.float64, np.int32):\n",
    "            high_threshold = self.hightemp\n",
    "        else:   # type is np array\n",
    "            high_threshold = np.array([])\n",
    "            while high_threshold.size < data_tx.size:\n",
    "                high_threshold = np.concatenate([high_threshold, self.hightemp])\n",
    "        if type(self.lowtemp) in (float, int, np.float64, np.int32):\n",
    "            low_threshold = self.lowtemp\n",
    "        else:   # type is np array\n",
    "            low_threshold = np.array([])\n",
    "            while low_threshold.size < data_tn.size:\n",
    "                low_threshold = np.concatenate([low_threshold, self.lowtemp])\n",
    "        tf_array_tx = data_tx >= high_threshold\n",
    "        tf_array_tn = data_tn >= low_threshold\n",
    "        return tf_array_tx * tf_array_tn\n",
    "    \n",
    "    def val_dist(self, datalist):\n",
    "        tfarray = self.tf_array(datalist)\n",
    "        tfarray = tfarray.reshape(tfarray.size//365, 365)\n",
    "        vals = np.apply_along_axis(count_runs, 1, tfarray, self.min_duration)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "    \n",
    "class Tempwave_highlow_duration(Hazard):\n",
    "    # Consecutive run of days with temp within a range. Duration in days of longest run in a year.\n",
    "    def __init__(self, hazname, hightemp, lowtemp, min_duration, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = 'tasmax+tasmin'\n",
    "        self.min_duration = min_duration\n",
    "        self.hightemp = hightemp\n",
    "        self.lowtemp = lowtemp\n",
    "        self.probmodel = 'Poisson'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "        \n",
    "        \n",
    "    def tf_array(self, datalist):\n",
    "        data_tx = datalist[0]\n",
    "        data_tn = datalist[1]\n",
    "        if type(self.hightemp) in (float, int, np.float64, np.int32):\n",
    "            high_threshold = self.hightemp\n",
    "        else:   # type is np array\n",
    "            high_threshold = np.array([])\n",
    "            while high_threshold.size < data_tx.size:\n",
    "                high_threshold = np.concatenate([high_threshold, self.hightemp])\n",
    "        if type(self.lowtemp) in (float, int, np.float64, np.int32):\n",
    "            low_threshold = self.lowtemp\n",
    "        else:   # type is np array\n",
    "            low_threshold = np.array([])\n",
    "            while low_threshold.size < data_tn.size:\n",
    "                low_threshold = np.concatenate([low_threshold, self.lowtemp])\n",
    "        tf_array_tx = data_tx >= high_threshold\n",
    "        tf_array_tn = data_tn >= low_threshold\n",
    "        return tf_array_tx * tf_array_tn\n",
    "    \n",
    "    def val_dist(self, datalist):\n",
    "        tfarray = self.tf_array(datalist)\n",
    "        tfarray = tfarray.reshape(tfarray.size//365, 365)\n",
    "        vals = np.apply_along_axis(longest_run, 1, tfarray)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "\n",
    "class ThresholdDays(Hazard):\n",
    "    # Num days (can be nonconsecutive) meeting some criterion in a year\n",
    "    def __init__(self, hazname, varname, var_threshold, want_max, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = varname\n",
    "        self.var_threshold = var_threshold\n",
    "        self.want_max = want_max\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "\n",
    "    def val_dist(self, datalist):\n",
    "        data = datalist[0]\n",
    "        if data.size % 365 != 0:\n",
    "            raise Exception('Data array length is not an integer multiple of 365')   \n",
    "        byyear = data.reshape(data.size // 365, 365)\n",
    "        \n",
    "        if self.want_max:\n",
    "            vals = np.sum(byyear >= self.var_threshold, axis=1)\n",
    "        else:\n",
    "            vals = np.sum(byyear <= self.var_threshold, axis=1)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "    \n",
    "class RangeDays(Hazard):\n",
    "    # Num days (can be nonconsecutive) in a year with some variable with a range\n",
    "    def __init__(self, hazname, varname, var_thresholdlow, var_thresholdhigh, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = varname\n",
    "        self.var_thresholdlow = var_thresholdlow\n",
    "        self.var_thresholdhigh = var_thresholdhigh\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "\n",
    "    def val_dist(self, datalist):\n",
    "        data = datalist[0]\n",
    "        byyear = data.reshape(data.size // 365, 365)\n",
    "        \n",
    "        vals = np.sum(np.logical_and(byyear >= self.var_thresholdlow, byyear <= self.var_thresholdhigh), axis=1)\n",
    "\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "class Annual_val(Hazard):\n",
    "    def __init__(self, hazname, varname, aggtype, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = varname\n",
    "        self.aggtype = aggtype\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "        \n",
    "    def val_dist(self, datalist):\n",
    "        data = datalist[0]\n",
    "        byyear = data.reshape(data.size//365, 365)\n",
    "        if self.aggtype == 'sum':\n",
    "            vals = np.sum(byyear, axis=1)\n",
    "        elif self.aggtype == 'mean':\n",
    "            vals = np.mean(byyear, axis=1)\n",
    "        elif self.aggtype == 'max':\n",
    "            vals = np.max(byyear, axis=1)\n",
    "        elif self.aggtype == 'min':\n",
    "            vals = np.min(byyear, axis=1)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "\n",
    "class Seasonal_val(Hazard):\n",
    "    # Sum, mean, max, or min of some variable value within a year\n",
    "    def __init__(self, hazname, varname, aggtype, startdate, enddate, southern_hem, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = varname\n",
    "        self.aggtype = aggtype\n",
    "        self.startdate = startdate\n",
    "        self.enddate = enddate\n",
    "        self.southern_hem = southern_hem\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "        \n",
    "    def val_dist(self, datalist):\n",
    "        data = datalist[0]\n",
    "        byyear = data.reshape(data.size//365, 365)\n",
    "        start_jday = d2j('1999-{0}'.format(self.startdate)) - [0, 182][int(self.southern_hem)]\n",
    "        end_jday = d2j('1999-{0}'.format(self.enddate)) - [0, 182][int(self.southern_hem)]\n",
    "        if end_jday < start_jday:\n",
    "            end_jday += 365\n",
    "        inseason_onerow = [((i >= start_jday)and(i <= end_jday)) for i in range(365)]\n",
    "        inseason = np.array([inseason_onerow]*(data.size//365))\n",
    "        byyear = byyear * inseason\n",
    "        if self.aggtype == 'sum':\n",
    "            vals = np.sum(byyear, axis=1)\n",
    "        elif self.aggtype == 'mean':\n",
    "            vals = np.mean(byyear, axis=1)\n",
    "        elif self.aggtype == 'max':\n",
    "            vals = np.max(byyear, axis=1)\n",
    "        elif self.aggtype == 'min':\n",
    "            vals = np.min(byyear, axis=1)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        #print(vals)\n",
    "        return result_dist\n",
    "\n",
    "class ARIDays(Hazard):\n",
    "    # Antecedent Rainfall Index, used for landslide risk\n",
    "    # Num days with heavy rainfall in the five prior days\n",
    "    # Must be combined with geological instability to get landslide risk\n",
    "    def __init__(self, hazname, ari95, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = 'pr'\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "\n",
    "    def val_dist(self, datalist):\n",
    "        data = datalist[0]\n",
    "        if data.size % 365 != 0:\n",
    "            raise Exception('Data array length is not an integer multiple of 365')   \n",
    "        byyear = data.reshape(data.size // 365, 365)\n",
    "        \n",
    "        aris = np.apply_along_axis(ari, 1, byyear)\n",
    "        vals = np.sum(aris >= ari95, axis=1)\n",
    "        \n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "    \n",
    "class WetbulbDays(Hazard):\n",
    "    # Days in a year (can be nonconsecutive) with wetbulb temp exceeding some value\n",
    "    def __init__(self, hazname, wbt_threshold, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = 'maxwetbulb'\n",
    "        self.wbt_threshold = wbt_threshold\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "    \n",
    "    def val_dist(self, datalist):\n",
    "        data_t = datalist[0]\n",
    "        data_h = datalist[1]\n",
    "        data = wetbulbtemp(data_t, data_h)\n",
    "        if data.size % 365 != 0:\n",
    "            raise Exception('Data array length is not an integer multiple of 365')\n",
    "        byyear = data.reshape(data.size//365, 365)\n",
    "        vals = np.sum(byyear >= self.wbt_threshold, axis=1)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "\n",
    "class DroughtSPIDays(Hazard):\n",
    "    # Meteorological drought hazard based on Standardized Precipitation Index\n",
    "    def __init__(self, hazname, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = 'pr'\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "    \n",
    "    def val_dist(self, datalist):\n",
    "        data = datalist[0]\n",
    "        if data.size % 365 != 0:\n",
    "            raise Exception('Data array length is not an integer multiple of 365')\n",
    "        \n",
    "        t=pd.date_range(start='1980-01-01', end='{0}-12-31'.format(1980 + (data.size//365) - 1), freq='D')\n",
    "        t = t[~((t.month == 2) & (t.day == 29))]\n",
    "        \n",
    "        try:\n",
    "            droughtdays = spei.spi(pd.Series(data, index=t)).to_numpy()\n",
    "        except:\n",
    "            return None\n",
    "        byyear = droughtdays.reshape(data.size // 365, 365)\n",
    "        \n",
    "        vals = np.sum(byyear <= -1.5, axis=1)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "    \n",
    "class DegreeDays(Hazard):\n",
    "    # Yearlong sum of positive deviations from reference temperature\n",
    "    def __init__(self, hazname, reftemp, want_max, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = 'tas'\n",
    "        self.reftemp = reftemp\n",
    "        self.extremeval = extremeval\n",
    "        self.want_max = want_max\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "\n",
    "    def val_dist(self, datalist):\n",
    "        data = datalist[0]\n",
    "        if data.size % 365 != 0:\n",
    "            raise Exception('Data array length is not an integer multiple of 365')   \n",
    "        byyear = data.reshape(data.size // 365, 365)\n",
    "        \n",
    "        if self.want_max:\n",
    "            vals = np.sum(np.round(np.maximum(0, byyear - self.reftemp)), axis=1)\n",
    "        else:\n",
    "            vals = np.sum(np.round(np.maximum(0, (-1 * byyear) + self.reftemp)), axis=1)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b21c38fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_observed_gee(varname, latlon, start_year, end_year, southern_hem=False):\n",
    "    # Get ERA5 data from Google Earth Engine\n",
    "    # Return numpy array in correct units, leapdays removed\n",
    "    def relhum(T, Tdp):\n",
    "        # relative humidity as percent on [0, 100]\n",
    "        # Used only if variable of interest is hurs\n",
    "        T = T.astype('float64')\n",
    "        Tdp = Tdp.astype('float64')\n",
    "        numerator = np.exp(17.625 * Tdp / (243.04 + Tdp))\n",
    "        denominator = np.exp(17.625 * T / (243.04 + T))\n",
    "        return 100 * numerator / denominator\n",
    "\n",
    "    def get_eradata(varname, southern_hem=False):\n",
    "        # Return numpy array in correct units, leapdays removed\n",
    "        dataset = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\n",
    "        gee_geom = ee.Geometry.Point((latlon[1], latlon[0]))\n",
    "        data_vars = dataset.select(varname).filter(ee.Filter.date('{0}-01-01'.format(start_year), '{0}-01-01'.format(end_year+ 1)))\n",
    "        success = False\n",
    "        while not success:\n",
    "            try:\n",
    "                d = data_vars.getRegion(gee_geom, 2500, 'epsg:4326').getInfo()\n",
    "                success = True\n",
    "            except:\n",
    "                print('\\nRetrying')\n",
    "        result = [i[4] for i in d[1:]]\n",
    "        return np.array(result)\n",
    "    \n",
    "    if varname == 'hurs':\n",
    "        success = False\n",
    "        era_dewpoint = get_eradata('dewpoint_2m_temperature')-273.15\n",
    "        era_maxtemp = get_eradata('maximum_2m_air_temperature')-273.15\n",
    "        hist_obs = relhum(era_maxtemp, era_dewpoint)\n",
    "    elif varname == 'pr':\n",
    "        hist_obs = get_eradata('total_precipitation') * 1000\n",
    "    elif varname == 'tasmax':\n",
    "        hist_obs = get_eradata('maximum_2m_air_temperature')-273.15\n",
    "    else:    # varname == 'tasmin'\n",
    "        hist_obs = get_eradata('minimum_2m_air_temperature')-273.15\n",
    "    return removeLeapDays(hist_obs, start_year, end_year, southern_hem)\n",
    "\n",
    "def get_modeled_gee(varname, scenario, model, lat, lon, southern_hem, start_year, end_year):\n",
    "    # Get NEX-GDDP-CMIP5 data from Google Earth Engine\n",
    "    # Return numpy array in correct units, uncalibrated, leapdays removed\n",
    "    dataset = ee.ImageCollection('NASA/GDDP-CMIP6').filter(ee.Filter.eq('model', model)).filter(ee.Filter.eq('scenario', scenario))\n",
    "    gee_geom = ee.Geometry.Point((lon, lat))\n",
    "    if start_year >= 2015 and not (start_year==2015 and southern_hem):\n",
    "        if southern_hem:\n",
    "            data_vars = dataset.select(varname).filter(ee.Filter.date('{0}-07-01'.format(start_year-1), '{0}-07-01'.format(end_year)))\n",
    "        else:\n",
    "            data_vars = dataset.select(varname).filter(ee.Filter.date('{0}-01-01'.format(start_year), '{0}-01-01'.format(end_year+ 1)))\n",
    "        result = [i[4] for i in data_vars.getRegion(gee_geom, 2500, 'epsg:4326').getInfo()[1:]]\n",
    "    else:\n",
    "        hist_dataset = ee.ImageCollection('NASA/GDDP-CMIP6').filter(ee.Filter.eq('model', model))\n",
    "        if southern_hem:\n",
    "            hist_part = hist_dataset.select(varname).filter(ee.Filter.eq('scenario', 'historical')).filter(ee.Filter.date('{0}-07-01'.format(start_year-1), '2015-01-01'))\n",
    "            if end_year >= 2015:\n",
    "                ssp_part = dataset.select(varname).filter(ee.Filter.eq('scenario', scenario)).filter(ee.Filter.date('2015-01-01', '{0}-07-01'.format(end_year)))\n",
    "        else:\n",
    "            hist_part = hist_dataset.select(varname).filter(ee.Filter.eq('scenario', 'historical')).filter(ee.Filter.date('{0}-01-01'.format(start_year), '2015-01-01'))\n",
    "            if end_year >= 2015:\n",
    "                ssp_part = dataset.select(varname).filter(ee.Filter.eq('scenario', scenario)).filter(ee.Filter.date('2015-01-01'.format(start_year-1), '{0}-01-01'.format(end_year+ 1)))\n",
    "        hist_result = [i[4] for i in hist_part.getRegion(gee_geom, 2500, 'epsg:4326').getInfo()[1:]]\n",
    "        if end_year >= 2015:\n",
    "            ssp_result = [i[4] for i in ssp_part.getRegion(gee_geom, 2500, 'epsg:4326').getInfo()[1:]]\n",
    "        else:\n",
    "            ssp_result = []\n",
    "        result = hist_result + ssp_result\n",
    "    d =  VARIABLES[varname]['nex_transform'](np.array(result))\n",
    "    return removeLeapDays(d, start_year, end_year, southern_hem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9066e5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdata(varnames, warmingscenario, lat, lon, best_models):\n",
    "    res = {varname: {} for varname in varnames}\n",
    "    for varname in varnames:\n",
    "        for model in best_models[varname]:\n",
    "            scenario = scenarioyears[model][warmingscenario][0]\n",
    "            start_year = scenarioyears[model][warmingscenario][1] - 4\n",
    "            end_year = scenarioyears[model][warmingscenario][1] + 5\n",
    "            res[varname][model] = (start_year, end_year, get_modeled_gee(varname, scenario, model, lat, lon, lat < 0, start_year, end_year))\n",
    "    return res\n",
    "\n",
    "def do_locationhazard(loc_id, hazard, datasets, latlon, scenario, calib_fxns):\n",
    "    lat, lon = latlon    \n",
    "    return loc_id, lat, lon, hazard.hazname, scenario, hazard.get_expectedval(latlon, datasets, calib_fxns), hazard.get_exceedanceprob(latlon, datasets, calib_fxns)\n",
    "    #return hazard.get_exceedanceprob(latlon, datasets, calib_fxns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f591121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dict helps with calculation of max wetbulb temp, which is the only hazard in this project\n",
    "# that is calculated with more than one modeled meteorological variable.\n",
    "\n",
    "SYNTHVARS = {\n",
    "    'maxwetbulb': {\n",
    "        'nex_varnames': ['tasmax', 'hurs'],\n",
    "        'era_varnames': ['maximum_2m_air_temperature', 'dewpoint_2m_temperature'],\n",
    "        'nex_fxn': lambda a, b: wetbulbtemp(a,b),\n",
    "        'era_fxn': lambda a, b: wetbulbtemp(a, relhum(a, b)),\n",
    "        'era_transform': [lambda x: x - 273.15, lambda x: x - 273.15]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66fef401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration functions are arrays of four arrays, each of which is for one seasonal quarter and contains approx 3000\n",
    "# numbers on [0,1]. These numbers describe the P-P plot for one meteorological variable, for historical values from ERA5\n",
    "# and from a NEX-GDDP-CMIP6 model for the same historical years. This fxn reads calib fxns for three models for each location\n",
    "# and stores them in a dict with location-id as keys.\n",
    "\n",
    "def get_calibfxns(varname, cityinfo):\n",
    "    cf = {loc_id: {} for loc_id in range(len(cityinfo))}\n",
    "    for varname in varname.split('+'):\n",
    "        with open('bmcf_{0}.txt'.format(varname), 'r') as ifile:\n",
    "            lines = ifile.readlines()\n",
    "            for linenum, line in enumerate(lines):\n",
    "                items = [i.strip() for i in line.split('\\t')]\n",
    "                if linenum % 3 == 0:\n",
    "                    cf[int(items[0])][varname] = {}\n",
    "                cf[int(items[0])][varname][items[1]] = json.loads(items[2])\n",
    "    return cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9af2291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario years are the years used for each warming scenario. Each model has a different 10-year scenario-year interval.\n",
    "scenarioyears = {}\n",
    "with open('scenarioyears_v2.csv', 'r') as ifile:\n",
    "    lines = ifile.readlines()\n",
    "    for line in lines:\n",
    "        items = [i.strip() for i in line.split(',')]\n",
    "        scenarioyears[items[0]] = {\n",
    "            'baseline': ('ssp245', 2015),\n",
    "            '1.5C': (items[1], int(items[2])),\n",
    "            '2.0C': (items[1], int(items[3])),\n",
    "            '3.0C': (items[1], int(items[4]))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4638c514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pr\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 CPU times: total: 1h 40min 53s\n",
      "Wall time: 2h 45min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = []\n",
    "for varname in ['tasmax', 'tas', 'pr', 'maxwetbulb']:\n",
    "    print()\n",
    "    print(varname)\n",
    "    calibfxns = get_calibfxns(varname, CITYLATLON)\n",
    "    for cityname in CITYLATLON:\n",
    "        lat, lon, loc_id = CITYLATLON[cityname]\n",
    "        loc_id = int(loc_id)\n",
    "        print(loc_id, end=' ')\n",
    "        latlon = (lat, lon)\n",
    "        if varname == 'tasmax':\n",
    "            tasmax90 = yearextreme_percentile('tasmax', 90, latlon, True)\n",
    "            HAZARDS = [\n",
    "                Tempwave_count('Heatwavecount tmax 90pctl 3plus', 'tasmax', 3, tasmax90, True, [1, 3, 5]),\n",
    "                Tempwave_duration('Heatwaveduration tmax 90pctl', 'tasmax', tasmax90, True, [20, 30, 40]),\n",
    "                ThresholdDays('Days warmer than 35', 'tasmax', 35, True, [10, 20, 30]),\n",
    "                ThresholdDays('Days warmer than 40', 'tasmax', 40, True, [10, 20, 30]),\n",
    "                ThresholdDays('Days warmer than than 95th pctle yearlong', 'tasmax', tasmax90, True, [60, 70, 80]),\n",
    "                Annual_val('Hottest annual temp', 'tasmax', 'max', [35, 40, 45]),\n",
    "            ]\n",
    "        elif varname == 'pr':\n",
    "            pr90 = yearextreme_percentile('pr', 90, latlon, True)\n",
    "            ari95 = get_ari95(latlon)\n",
    "            HAZARDS = [\n",
    "                Annual_val('Highest daily precip', 'pr', 'max', [500, 1000, 2000]),\n",
    "                ThresholdDays('Days precip gte 90th pctl', 'pr', pr90, True, [20, 30, 40]),\n",
    "                ARIDays('ARI days gte ari95', ari95, [5, 10, 20]),\n",
    "                DroughtSPIDays('Drought days SPI lte -2', [100, 140, 180])\n",
    "            ]\n",
    "        elif varname == 'tas':\n",
    "            HAZARDS = [\n",
    "                DegreeDays('CDD21', 21, True, [2000, 3000, 4000]),\n",
    "                RangeDays('arbovirus temp', 'tas', 26, 29, [30, 60, 90]), #https://journals.plos.org/plosntds/article?id=10.1371/journal.pntd.0005568\n",
    "                RangeDays('malaria temp', 'tas', 22.9, 27.8, [30, 60, 90]), # https://malariajournal.biomedcentral.com/articles/10.1186/s12936-020-03224-6\n",
    "            ]\n",
    "        elif varname == 'maxwetbulb':\n",
    "            HAZARDS = [\n",
    "                WetbulbDays('Wetbulb days gte 31', 31, [10, 25, 30]),\n",
    "            ]\n",
    "        \n",
    "        warming_scenarios = ['baseline','1.5C', '2.0C', '3.0C']\n",
    "        varnames = []\n",
    "        for hazard in HAZARDS:\n",
    "            if hazard.varname == varname:\n",
    "                for scenario in warming_scenarios:\n",
    "                    if varname in SYNTHVARS:\n",
    "                        varnames += SYNTHVARS[varname]['nex_varnames']\n",
    "                    else:\n",
    "                        varnames += hazard.varname.split('+')\n",
    "        varnames = list(set(varnames))\n",
    "        allds = {scenario: getdata(varnames, scenario, lat, lon, {vn: list(calibfxns[loc_id][varname].keys()) for vn in varnames})\n",
    "                    for scenario in warming_scenarios}\n",
    "        for hazard in HAZARDS:\n",
    "            if hazard.varname == varname:\n",
    "                for scenario in warming_scenarios:\n",
    "                    if varname in SYNTHVARS:\n",
    "                        varnames = SYNTHVARS[varname]['nex_varnames']\n",
    "                    else:\n",
    "                        varnames = hazard.varname.split('+')\n",
    "                    datasets = allds[scenario]\n",
    "                    res = do_locationhazard(loc_id, hazard, datasets, (lat, lon), scenario, calibfxns[loc_id])\n",
    "                    rlocid, rlat, rlon, rhazname, rscenario, rev, rprob = res\n",
    "                    with open('RESULTS.csv', 'a') as ifile:\n",
    "                        for rank, rmod in enumerate(list(rev.keys())):\n",
    "                            if len(rmod.split('+')) > 1:\n",
    "                                displaymod = rmod.split('+')[0]\n",
    "                            else:\n",
    "                                displaymod = rmod\n",
    "                            ifile.write('{0},{1},{2},{3},{4},'.format(rlocid, rlat, rlon, rhazname, rscenario))\n",
    "                            ifile.write('{0},{1},{2},{3},{4}'.format(displaymod, rank + 1, rev[rmod][0], rev[rmod][1], rev[rmod][2]))\n",
    "                            for rexval in rprob[rmod]:\n",
    "                                ifile.write(',{0},{1},{2},{3}'.format(rexval, rprob[rmod][rexval][0], rprob[rmod][rexval][1], rprob[rmod][rexval][2]))\n",
    "                            ifile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cff76c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (halfdegenv)",
   "language": "python",
   "name": "halfdegenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
