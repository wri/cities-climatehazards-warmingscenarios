{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abdba210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "%matplotlib inline\n",
    "import math\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "from scipy import stats\n",
    "import scipy\n",
    "\n",
    "import datetime, calendar\n",
    "import spei\n",
    "\n",
    "#ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "254785d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIST_START = 1980\n",
    "HIST_END = 2014\n",
    "\n",
    "PERCENTILE_STARTYEAR = 1980\n",
    "PERCENTILE_ENDYEAR = 2019\n",
    "\n",
    "NUM_BEST_MODELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66e61532",
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLES = {\n",
    "    'tas': {\n",
    "        'era_varname': 'mean_2m_air_temperature',\n",
    "        'nex_transform': lambda x: x - 273.5,\n",
    "        'era_transform': lambda x: x - 273.5\n",
    "    },\n",
    "    'tasmax': {\n",
    "        'era_varname': 'maximum_2m_air_temperature',\n",
    "        'nex_transform': lambda x: x - 273.5,\n",
    "        'era_transform': lambda x: x - 273.5\n",
    "    },\n",
    "    'tasmin': {\n",
    "        'era_varname': 'minimum_2m_air_temperature',\n",
    "        'nex_transform': lambda x: x - 273.5,\n",
    "        'era_transform': lambda x: x - 273.5\n",
    "    },\n",
    "    'pr': {\n",
    "        'era_varname': 'total_precipitation',\n",
    "        'nex_transform': lambda x: x * 86400,\n",
    "        'era_transform': lambda x: x * 1000\n",
    "    },\n",
    "    'hurs': {\n",
    "        'era_varname': None,\n",
    "       'nex_transform': lambda x: x,\n",
    "        'era_transform': lambda x: x\n",
    "    },\n",
    "    'maxwetbulb': {\n",
    "        'era_varname': None,\n",
    "       'nex_transform': lambda x: x,\n",
    "        'era_transform': lambda x: x\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af292bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = ['MIROC-ES2L', 'UKESM1-0-LL', 'EC-Earth3-Veg-LR', 'GFDL-ESM4', 'INM-CM5-0', 'MRI-ESM2-0', 'FGOALS-g3', 'IPSL-CM6A-LR', 'CanESM5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f97dfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "CITYLATLON = {}\n",
    "with open('ghsl_500k.csv', 'r') as ifile:\n",
    "    for line in ifile.readlines():\n",
    "        items = [i.strip() for i in line.split(',')]\n",
    "        CITYLATLON['city_{0}'.format(items[0])] = (float(items[2]), float(items[3]), int(items[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "001545c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calendardate_percentiles(nex_varname, q, latlon, sh_hem=False):\n",
    "    hist_start = PERCENTILE_STARTYEAR\n",
    "    hist_end = PERCENTILE_ENDYEAR\n",
    "    allyears = []\n",
    "    for year in range(hist_start, hist_end):\n",
    "        allyears.append(get_observed_gee(nex_varname, latlon, start_year=year, end_year=year, southern_hem=False))\n",
    "    if not sh_hem:\n",
    "        return np.percentile(np.vstack(allyears), q, axis=0)\n",
    "    else:\n",
    "        res = np.percentile(np.vstack(allyears), q, axis=0)\n",
    "        return np.concatenate([res[152:], res[:152]])\n",
    "\n",
    "def wholeyear_percentile(nex_varname, q, latlon):\n",
    "    if not nex_varname == 'ari':\n",
    "        hist_start = PERCENTILE_STARTYEAR\n",
    "        hist_end = PERCENTILE_ENDYEAR\n",
    "        allyears = []\n",
    "        for year in range(hist_start, hist_end):\n",
    "            allyears.append(get_observed_gee(nex_varname, latlon, start_year=year, end_year=year, southern_hem=False))\n",
    "        return np.percentile(np.concatenate(allyears).flatten(), q)\n",
    "    else:\n",
    "        hist_start = PERCENTILE_STARTYEAR\n",
    "        hist_end = PERCENTILE_ENDYEAR\n",
    "        allyears = []\n",
    "        for year in range(hist_start, hist_end):\n",
    "            allyears.append(get_observed_gee('pr', latlon, start_year=year, end_year=year, southern_hem=False))\n",
    "        ari_data = ari(np.concatenate(allyears).flatten())\n",
    "        return np.percentile(ari_data, 95)\n",
    "\n",
    "def yearextreme_percentile(nex_varname, q, latlon, wantmax):\n",
    "    hist_start = PERCENTILE_STARTYEAR\n",
    "    hist_end = PERCENTILE_ENDYEAR\n",
    "    allyears = get_observed_gee(nex_varname, latlon, hist_start, hist_end, southern_hem=False)\n",
    "    return np.percentile(allyears, q)\n",
    "\n",
    "def d2j(datestring):\n",
    "    d = datetime.date.fromisoformat(datestring)\n",
    "    jday = d.timetuple().tm_yday\n",
    "    if calendar.isleap(d.year) and jday > 59:\n",
    "        jday -= 1\n",
    "    return jday\n",
    "\n",
    "def removeLeapDays(arr, start_year, end_year, southern_hem):\n",
    "    indices_to_remove = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        if calendar.isleap(year):\n",
    "            indices_to_remove.append(((year-start_year) * 365) + [0,183][int(southern_hem)] + len(indices_to_remove) + 59)\n",
    "    return np.delete(arr, indices_to_remove)\n",
    "\n",
    "def get_rmsd(d1, d2):\n",
    "    c1 = seasonal_means(d1)\n",
    "    c2 = seasonal_means(d2)\n",
    "    return np.sqrt(np.mean(np.sum((c1 - c2)**2)))\n",
    "\n",
    "def count_runs(tf_array, min_runsize):\n",
    "    falses = np.zeros(tf_array.shape[0]).reshape((tf_array.shape[0],1))\n",
    "    extended_a = np.concatenate([[0], tf_array, [0]])\n",
    "    df = np.diff(extended_a)\n",
    "    starts = np.nonzero(df == 1)[0]\n",
    "    ends = np.nonzero(df == -1)[0]\n",
    "    count = 0\n",
    "    for idx in range(starts.size):\n",
    "        if ends[idx] - starts[idx] >= min_runsize:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def longest_run(tf_array):\n",
    "    if np.sum(tf_array) == 0:\n",
    "        return 0\n",
    "    falses = np.zeros(tf_array.shape[0]).reshape((tf_array.shape[0],1))\n",
    "    extended_a = np.concatenate([[0], tf_array, [0]])\n",
    "    df = np.diff(extended_a)\n",
    "    starts = np.nonzero(df == 1)[0]\n",
    "    ends = np.nonzero(df == -1)[0]\n",
    "    durations = ends - starts\n",
    "    return max(durations)\n",
    "    \n",
    "def quarters(d, start_year, end_year, southern_hem=False):\n",
    "    #Takes multi-year array and returns data reorganized into quarters\n",
    "    q2 = []  # 60-151\n",
    "    q3 = []  # 152-243\n",
    "    q4 = []  # 244-334\n",
    "    q1 = []  # 335-59\n",
    "    if not southern_hem:\n",
    "        jan1_idx = 365\n",
    "        for year in range(start_year, end_year):\n",
    "            tmp = np.concatenate((d[jan1_idx - 365 : jan1_idx - 365 + 60], d[jan1_idx + 335 : jan1_idx + 365]), axis=0)\n",
    "            q1.append(tmp)\n",
    "            q2.append(d[jan1_idx + 60 : jan1_idx + 152])\n",
    "            q3.append(d[jan1_idx + 152 : jan1_idx + 244])\n",
    "            q4.append(d[jan1_idx + 244 : jan1_idx + 335])\n",
    "\n",
    "            jan1_idx += 365 + [0, 0][int(False and calendar.isleap(year))]\n",
    "        mam_res = np.vstack(q2)\n",
    "        jja_res = np.vstack(q3)\n",
    "        son_res = np.vstack(q4)\n",
    "        djf_res = np.vstack(q1)\n",
    "    else:\n",
    "        jul1_idx = 365\n",
    "        for year in range(start_year, end_year):\n",
    "            tmp = np.concatenate((d[jul1_idx - 365 : jul1_idx - 365 + 60], d[jul1_idx + 335 : jul1_idx + 365]), axis=0)\n",
    "            q3.append(tmp)\n",
    "            q4.append(d[jul1_idx + 60 : jul1_idx + 152])\n",
    "            q1.append(d[jul1_idx + 152 : jul1_idx + 244])\n",
    "            q2.append(d[jul1_idx + 244 : jul1_idx + 335])\n",
    "\n",
    "            jul1_idx += 365 + [0, 0][int(False and calendar.isleap(year))]\n",
    "        mam_res = np.vstack(q4)\n",
    "        jja_res = np.vstack(q1)\n",
    "        son_res = np.vstack(q2)\n",
    "        djf_res = np.vstack(q3)\n",
    "    return mam_res, jja_res, son_res, djf_res\n",
    "    \n",
    "def seasonal_means(d):\n",
    "    q = quarters(d, HIST_START, HIST_END)\n",
    "    return np.array([np.mean(q[0], axis=1), np.mean(q[1], axis=1), np.mean(q[2], axis=1), np.mean(q[3], axis=1)])\n",
    "\n",
    "def calibration_function(hist_obs, hist_mod):\n",
    "# Calibration functions are P-P plots of historical and modeled values\n",
    "\n",
    "    source = np.sort(hist_obs.flatten())\n",
    "    target= np.sort(hist_mod.flatten())\n",
    "   \n",
    "    if (np.max(source) == 0 and np.min(source) == 0):\n",
    "        return np.arange(0, target.size) / target.size\n",
    "    if (np.max(target) == 0 and np.min(target) == 0):\n",
    "        return np.arange(0, source.size) / source.size\n",
    "    new_indices = []\n",
    "\n",
    "    for target_idx, target_value in enumerate(target):\n",
    "        if target_idx < len(source):\n",
    "            source_value = source[target_idx]\n",
    "            if source_value > target[-1]:\n",
    "                new_indices.append(target.size - 1)\n",
    "            else:\n",
    "                new_indices.append(np.argmax(target >= source_value))\n",
    "    return np.array(new_indices) / source.size\n",
    "\n",
    "def calibrate_component(uncalibrated_data, calibration_fxn):\n",
    "    N = len(uncalibrated_data)\n",
    "    unsorted_uncalib = [(i, idx) for idx, i in enumerate(uncalibrated_data)]\n",
    "    sorted_uncalib = sorted(unsorted_uncalib)\n",
    "    result = [0] * N\n",
    "    for j in range(N):\n",
    "        X_j = j / (N + 1)\n",
    "        Y_jprime = calibration_fxn[math.floor(X_j * len(calibration_fxn))]\n",
    "        jprime = math.floor(Y_jprime * (N + 1))\n",
    "        result[sorted_uncalib[j][1]] = sorted_uncalib[min(len(sorted_uncalib)-1, jprime)][0]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def calibrate(uncalibrated_data, calibration_fxn):\n",
    "    mam = []\n",
    "    jja = []\n",
    "    son = []\n",
    "    djf = []\n",
    "    mam_idx = []\n",
    "    jja_idx = []\n",
    "    son_idx = []\n",
    "    djf_idx = []\n",
    "    for idx, i in enumerate(uncalibrated_data):\n",
    "        if idx % 365 >= 60 and idx % 365 < 152:\n",
    "            mam.append(uncalibrated_data[idx])\n",
    "            mam_idx.append(idx)\n",
    "        elif idx % 365 >= 152 and idx % 365 < 244:\n",
    "            jja.append(uncalibrated_data[idx])\n",
    "            jja_idx.append(idx)\n",
    "        elif idx % 365 >= 244 and idx % 365 < 335:\n",
    "            son.append(uncalibrated_data[idx])\n",
    "            son_idx.append(idx)\n",
    "        else:\n",
    "            djf.append(uncalibrated_data[idx])\n",
    "            djf_idx.append(idx)\n",
    "    \n",
    "    mam_calib = calibrate_component(np.array(mam), calibration_fxn[0])\n",
    "    jja_calib = calibrate_component(np.array(jja), calibration_fxn[1])\n",
    "    son_calib = calibrate_component(np.array(son), calibration_fxn[2])\n",
    "    djf_calib = calibrate_component(np.array(djf), calibration_fxn[3])\n",
    "    \n",
    "    result = [0] * len(uncalibrated_data)\n",
    "    for i in range(len(mam_idx)):\n",
    "        result[mam_idx[i]] = mam_calib[i]\n",
    "    for i in range(len(jja_idx)):\n",
    "        result[jja_idx[i]] = jja_calib[i]\n",
    "    for i in range(len(son_idx)):\n",
    "        result[son_idx[i]] = son_calib[i]\n",
    "    for i in range(len(djf_idx)):\n",
    "        result[djf_idx[i]] = djf_calib[i]\n",
    "\n",
    "    return np.array(result)\n",
    "\n",
    "def ari(yeardata):\n",
    "#    Antecedent rainfall index\n",
    "#    Used to estimate landslide risk\n",
    "    ARI_WEIGHTS = np.array([\n",
    "        0.013499274414000246,\n",
    "        0.01837401239683367,\n",
    "        0.026458577851440485,\n",
    "        0.041341527892875755,\n",
    "        0.07349604958733467,\n",
    "        0.16536611157150302,\n",
    "        0.6614644462860121\n",
    "    ])\n",
    "    def ari_7day(sevendayrain):\n",
    "        return np.dot(sevendayrain, ARI_WEIGHTS)\n",
    "    \n",
    "    res = []\n",
    "    for start_idx in range(yeardata.size-7):\n",
    "        res.append(ari_7day(yeardata[start_idx:start_idx+7]))\n",
    "    return res\n",
    "\n",
    "def get_ari95(latlon):\n",
    "\n",
    "#    ARI95 = ee.Image('users/tedwongwri/dataportal/landslide/ARI95')\n",
    "#    geom = ee.Geometry.Point((latlon[1], latlon[0]))\n",
    "#    res = ARI95.reduceRegion(ee.Reducer.mean(), geom, 10, 'epsg:4326').getInfo()['b1']\n",
    "#    return res\n",
    "    return wholeyear_percentile('ari', 95, latlon)\n",
    "        \n",
    "\n",
    "def wetbulbtemp(T, RH):\n",
    "# JA Knox et al. 2017. Two simple and accurate approximations for wet-bulb\n",
    "# temperature in moist conditions, with forecasting applications. Bull. Am.\n",
    "# Meteorol. Soc. 98(9): 1897-1906. doi:10.1175/BAMS-D-16-0246.1\n",
    "    T = T.astype(np.float64)\n",
    "    rh_percent = RH.astype(np.float64)\n",
    "    return T * np.arctan(0.151977 * np.sqrt(rh_percent + 8.313659)) + np.arctan(T + rh_percent) - np.arctan(rh_percent - 1.676331) + ((0.00391838 * ((rh_percent)**(3/2))) * np.arctan(0.023101 * rh_percent)) - 4.686035        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05aa86a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hazard:\n",
    "    def get_percentile(self, latlon, q):\n",
    "        southern_hem = int(latlon[0] < 0)\n",
    "        era_data = {}\n",
    "        scenario_years = {}\n",
    "        varnames = self.varname.split('+')\n",
    "        for varname in varnames:\n",
    "            era_data[varname] = get_observed_gee(varname, latlon, PERCENTILE_STARTYEAR, PERCENTILE_ENDYEAR, southern_hem)\n",
    "        countdist = self.val_dist([ed for ed in era_data])\n",
    "        return np.percentile(countdist, q)\n",
    "    \n",
    "    def get_expectedval(self, latlon, datasets, calib_fxns):\n",
    "        # Returns mean estimate (and 95% conf interval) of expected value of indicator value\n",
    "        # datasets is dict[varname][modelname]: (scenario_startyear, scenario_endyear, float_array)\n",
    "        # calib_fxns is dict[varname][modelname]: float_array\n",
    "        # returns dict[modelname]: confidence_interval_low, mean, confidence_interval_high\n",
    "        southern_hem = int(latlon[0] < 0)\n",
    "        fut_mod = {}\n",
    "        scenario_years = {}\n",
    "        varnames = list(datasets.keys())\n",
    "        \n",
    "        \n",
    "        \n",
    "        for varname in varnames:\n",
    "            for model in calib_fxns[self.varname].keys():\n",
    "                ds = datasets[varname][model][2]\n",
    "                scenario_years[model] = (datasets[varname][model][0], datasets[varname][model][1])\n",
    "                fut_mod[(varname, model)] = ds\n",
    "        best_models = []\n",
    "        for idx in range(NUM_BEST_MODELS):\n",
    "            # Modelnames that are modelname+modelname are deprecated.\n",
    "            # modelplus.split('+') will always be a one-element list.\n",
    "            modelplus = '+'.join([list(calib_fxns[self.varname].keys())[idx] for varname in varnames])\n",
    "            best_models.append(modelplus)\n",
    "\n",
    "        para_res = {}\n",
    "        for modelplus in best_models:\n",
    "            start_year = scenario_years[modelplus.split('+')[0]][0]\n",
    "            end_year = scenario_years[modelplus.split('+')[0]][1]\n",
    "            numbins = end_year - start_year + 1\n",
    "            calib_data = []\n",
    "            for idx, varname in enumerate(varnames):\n",
    "                model = modelplus.split('+')[idx]\n",
    "                calib_data.append(np.array(calibrate(fut_mod[(varname, model)], calib_fxns[self.varname][model]))) # Convert the uncalibrated data into calibrated data\n",
    "            countdist = self.val_dist([cd[[0,152][int(not southern_hem)]:[len(cd),-213][int(not southern_hem)]] for cd in calib_data]) # Call the indicator's val_dist fxn to get dict[indicator_value]: value_count\n",
    "            if countdist is None: # When the drought module fails to converge on a result, the drought val_dist method returns None\n",
    "                para_res[modelplus] = None\n",
    "            else:\n",
    "                observed_vals = np.array(list(countdist.keys()))\n",
    "                cdist = {} # Define bin centers and widths.\n",
    "                minval = observed_vals[0]\n",
    "                maxval = observed_vals[-1]\n",
    "                D = (maxval - minval) / (numbins - 1)\n",
    "                for i in range(numbins):\n",
    "                    centerval = minval + (i * D)\n",
    "                    cdist[centerval] = 0\n",
    "                for count in countdist:\n",
    "                    for centerval in cdist:\n",
    "                        if count >= centerval - (D/2) and count < centerval + (D/2):\n",
    "                            cdist[centerval] += 1\n",
    "                alpha = np.array(list(cdist.values())) + (1/numbins) # alpha is parameter for Dirichlet\n",
    "                res = []\n",
    "                for i in range(10000):\n",
    "                    dirich_samp = np.random.dirichlet(alpha, 1) # Get Dirichlet samples. These are probability vectors.\n",
    "                    mult_samp = np.random.multinomial(end_year - start_year + 1, dirich_samp[0], 1)[0] # Use Dirichlet samples to generate multinomial distributions, and sample from them to get event-count vectors.\n",
    "                    res.append(sum([list(cdist.keys())[j] * mult_samp[j] for j in range(len(list(cdist.keys())))]) / (end_year - start_year + 1)) # Store expected values from the samples.\n",
    "                res = np.array(res)\n",
    "                para_res[modelplus] = res\n",
    "        result = {}\n",
    "        for modelplus in best_models:\n",
    "            if para_res[modelplus] is None:\n",
    "                result[modelplus] = [-9999, -9999, -9999]\n",
    "            else:\n",
    "                result[modelplus] = [np.mean(para_res[modelplus])- (1.96 * np.std(para_res[modelplus])), np.mean(para_res[modelplus]), np.mean(para_res[modelplus]) + (1.96 * np.std(para_res[modelplus]))]\n",
    "        return result\n",
    "    \n",
    "    def get_exceedanceprob(self, latlon, datasets, calib_fxns):\n",
    "        # Returns mean estimate (and 95% conf interval) of probability that indicator equals or exceeds threshold value\n",
    "        # datasets is dict[varname][modelname]: (scenario_startyear, scenario_endyear, float_array)\n",
    "        # calib_fxns is dict[varname][modelname]: float_array\n",
    "        # returns dict[modelname][threshold_value]: confidence_interval_low, mean, confidence_interval_high\n",
    "        \n",
    "        def floorceiling_prob(p):\n",
    "            # Enforces probs must be in interval [0, 1]\n",
    "            return max(0, min(1, p))\n",
    "        \n",
    "        southern_hem = int(latlon[0] < 0)\n",
    "        fut_mod = {}\n",
    "        scenario_years = {}\n",
    "        varnames = list(datasets.keys())\n",
    "        for varname in varnames:\n",
    "            for model in calib_fxns[self.varname].keys():\n",
    "                ds = datasets[varname][model][2]\n",
    "                scenario_years[model] = (datasets[varname][model][0], datasets[varname][model][1])\n",
    "                fut_mod[(varname, model)] = ds\n",
    "        best_models = []\n",
    "        for idx in range(NUM_BEST_MODELS):\n",
    "            # Modelnames that are modelname+modelname are deprecated.\n",
    "            # modelplus.split('+') will always be a one-element list.\n",
    "            modelplus = '+'.join([list(calib_fxns[self.varname].keys())[idx] for varname in varnames])\n",
    "            best_models.append(modelplus)\n",
    "\n",
    "        prob_res = {}\n",
    "        for modelplus in best_models:\n",
    "            prob_res[modelplus] = {}\n",
    "            start_year = scenario_years[modelplus.split('+')[0]][0]\n",
    "            end_year = scenario_years[modelplus.split('+')[0]][1]\n",
    "            calib_data = []\n",
    "            for idx, varname in enumerate(varnames):\n",
    "                model = modelplus.split('+')[idx]\n",
    "                calib_data.append(np.array(calibrate(fut_mod[(varname, model)], calib_fxns[self.varname][model]))) # Convert the uncalibrated data into calibrated data\n",
    "            countdist = self.val_dist([cd[[0,152][int(not southern_hem)]:[len(cd),-213][int(not southern_hem)]] for cd in calib_data]) # Call the indicator's val_dist fxn to get dict[indicator_value]: value_count\n",
    "            for exval in self.extremeval:\n",
    "                if countdist is None: # When the drought module fails to converge on a result, the drought val_dist method returns None\n",
    "                    prob_res[modelplus][exval] = [-9999, -9999, -9999]\n",
    "                else:\n",
    "                    count = sum([countdist[val] * int((val >= exval and self.exceed_is_gte) or (val <= exval and not self.exceed_is_gte)) for val in countdist])\n",
    "                    N = end_year - start_year + 1\n",
    "                    if self.probmodel == 'Poisson':\n",
    "                        meanprob = (count + 0.5) / N\n",
    "                        stdprob = np.sqrt((2 * count) + 1) / N\n",
    "                    else: # self.probmodel == 'binomial':\n",
    "                        meanprob = ((count + 0.5) * N / (N + 1)) / N\n",
    "                        stdprob = (np.sqrt((N * (count + 0.5) * (N - count + 0.5) * ((2 * N) + 1)) / ((N + 2) * (N + 1) * (N + 1)))) / N\n",
    "                    prob_res[modelplus][exval] = [floorceiling_prob(meanprob - (1.96 * stdprob)), floorceiling_prob(meanprob), floorceiling_prob(meanprob + (1.96 * stdprob))]\n",
    "       \n",
    "        return prob_res\n",
    "\n",
    "class ARIDays(Hazard):\n",
    "    def __init__(self, hazname, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = 'pr'\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "\n",
    "    def val_dist(self, datalist):\n",
    "        data = datalist[0]\n",
    "        if data.size % 365 != 0:\n",
    "            raise Exception('Data array length is not an integer multiple of 365')   \n",
    "        byyear = data.reshape(data.size // 365, 365)\n",
    "        \n",
    "        vals = apply_along_axis(ari, data, axis=1)\n",
    "        \n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "    \n",
    "class WetbulbDays(Hazard):\n",
    "    def __init__(self, hazname, wbt_threshold, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = 'maxwetbulb'\n",
    "        self.wbt_threshold = wbt_threshold\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "    \n",
    "    def val_dist(self, datalist):\n",
    "        data_t = datalist[0]\n",
    "        data_h = datalist[1]\n",
    "        data = wetbulbtemp(data_t, data_h)\n",
    "        if data.size % 365 != 0:\n",
    "            raise Exception('Data array length is not an integer multiple of 365')\n",
    "        byyear = data.reshape(data.size//365, 365)\n",
    "        vals = np.sum(byyear >= self.wbt_threshold, axis=1)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "\n",
    "class DroughtSPIDays(Hazard):\n",
    "    def __init__(self, hazname, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = 'pr'\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "    \n",
    "    def val_dist(self, datalist):\n",
    "        data = datalist[0]\n",
    "        if data.size % 365 != 0:\n",
    "            raise Exception('Data array length is not an integer multiple of 365')\n",
    "        \n",
    "        t=pd.date_range(start='1980-01-01', end='{0}-12-31'.format(1980 + (data.size//365) - 1), freq='D')\n",
    "        t = t[~((t.month == 2) & (t.day == 29))]\n",
    "        \n",
    "        droughtdays = spei.spi(pd.Series(data, index=t)).to_numpy()\n",
    "        byyear = droughtdays.reshape(data.size // 365, 365)\n",
    "        \n",
    "        vals = np.sum(byyear <= -2, axis=1)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "\n",
    "class Tempwave_count(Hazard):\n",
    "    def __init__(self, hazname, varname, min_duration, threshold, tf_gte, extremeval):\n",
    "        if type(threshold) == np.ndarray and threshold.size % 365 != 0:\n",
    "            raise Exception('Comparison array length is not an integer multiple of 365')\n",
    "        self.hazname = hazname\n",
    "        self.varname = varname\n",
    "        self.tf_gte = tf_gte\n",
    "        self.min_duration = min_duration\n",
    "        self.threshold = threshold  # May be scalar or 365-long array\n",
    "        self.probmodel = 'Poisson'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "        \n",
    "    def tf_array(self, datalist):\n",
    "        data = datalist[0]\n",
    "        if self.tf_gte:\n",
    "            return data >= self.threshold\n",
    "        else:\n",
    "            return data <= self.threshold\n",
    "\n",
    "    def val_dist(self, datalist):\n",
    "        tfarray = self.tf_array(datalist)\n",
    "        tfarray = tfarray.reshape(tfarray.size//365, 365)\n",
    "        vals = np.apply_along_axis(count_runs, 1, tfarray, self.min_duration)\n",
    "        \n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "    \n",
    "class Tempwave_duration(Hazard):\n",
    "    def __init__(self, hazname, varname, threshold, tf_gte, extremeval):\n",
    "        if type(threshold) == np.ndarray and threshold.size % 365 != 0:\n",
    "            raise Exception('Comparison array length is not an integer multiple of 365')\n",
    "        self.hazname = hazname\n",
    "        self.varname = varname\n",
    "        self.tf_gte = tf_gte\n",
    "        self.threshold = threshold  # May be scalar or 365-long array\n",
    "        self.probmodel = 'Poisson'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "        \n",
    "    def tf_array(self, datalist):\n",
    "        data = datalist[0]\n",
    "        if self.tf_gte:\n",
    "            return data >= self.threshold\n",
    "        else:\n",
    "            return data <= self.threshold\n",
    "\n",
    "    def val_dist(self, datalist):\n",
    "        tfarray = self.tf_array(datalist)\n",
    "        tfarray = tfarray.reshape(tfarray.size//365, 365)\n",
    "        vals = np.apply_along_axis(longest_run, 1, tfarray)\n",
    "        \n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "\n",
    "class Tempwave_highlow_count(Hazard):\n",
    "    def __init__(self, hazname, hightemp, lowtemp, min_duration, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = 'tasmax+tasmin'\n",
    "        self.min_duration = min_duration\n",
    "        self.hightemp = hightemp\n",
    "        self.lowtemp = lowtemp\n",
    "        self.probmodel = 'Poisson'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "        \n",
    "    def tf_array(self, datalist):\n",
    "        data_tx = datalist[0]\n",
    "        data_tn = datalist[1]\n",
    "        if type(self.hightemp) in (float, int, np.float64, np.int32):\n",
    "            high_threshold = self.hightemp\n",
    "        else:   # type is np array\n",
    "            high_threshold = np.array([])\n",
    "            while high_threshold.size < data_tx.size:\n",
    "                high_threshold = np.concatenate([high_threshold, self.hightemp])\n",
    "        if type(self.lowtemp) in (float, int, np.float64, np.int32):\n",
    "            low_threshold = self.lowtemp\n",
    "        else:   # type is np array\n",
    "            low_threshold = np.array([])\n",
    "            while low_threshold.size < data_tn.size:\n",
    "                low_threshold = np.concatenate([low_threshold, self.lowtemp])\n",
    "        tf_array_tx = data_tx >= high_threshold\n",
    "        tf_array_tn = data_tn >= low_threshold\n",
    "        return tf_array_tx * tf_array_tn\n",
    "    \n",
    "    def val_dist(self, datalist):\n",
    "        tfarray = self.tf_array(datalist)\n",
    "        tfarray = tfarray.reshape(tfarray.size//365, 365)\n",
    "        vals = np.apply_along_axis(count_runs, 1, tfarray, self.min_duration)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "    \n",
    "class Tempwave_highlow_duration(Hazard):\n",
    "    def __init__(self, hazname, hightemp, lowtemp, min_duration, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = 'tasmax+tasmin'\n",
    "        self.min_duration = min_duration\n",
    "        self.hightemp = hightemp\n",
    "        self.lowtemp = lowtemp\n",
    "        self.probmodel = 'Poisson'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "        \n",
    "        \n",
    "    def tf_array(self, datalist):\n",
    "        data_tx = datalist[0]\n",
    "        data_tn = datalist[1]\n",
    "        if type(self.hightemp) in (float, int, np.float64, np.int32):\n",
    "            high_threshold = self.hightemp\n",
    "        else:   # type is np array\n",
    "            high_threshold = np.array([])\n",
    "            while high_threshold.size < data_tx.size:\n",
    "                high_threshold = np.concatenate([high_threshold, self.hightemp])\n",
    "        if type(self.lowtemp) in (float, int, np.float64, np.int32):\n",
    "            low_threshold = self.lowtemp\n",
    "        else:   # type is np array\n",
    "            low_threshold = np.array([])\n",
    "            while low_threshold.size < data_tn.size:\n",
    "                low_threshold = np.concatenate([low_threshold, self.lowtemp])\n",
    "        tf_array_tx = data_tx >= high_threshold\n",
    "        tf_array_tn = data_tn >= low_threshold\n",
    "        return tf_array_tx * tf_array_tn\n",
    "    \n",
    "    def val_dist(self, datalist):\n",
    "        tfarray = self.tf_array(datalist)\n",
    "        tfarray = tfarray.reshape(tfarray.size//365, 365)\n",
    "        vals = np.apply_along_axis(longest_run, 1, tfarray)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "\n",
    "class ThresholdDays(Hazard):\n",
    "    def __init__(self, hazname, varname, var_threshold, want_max, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = varname\n",
    "        self.var_threshold = var_threshold\n",
    "        self.want_max = want_max\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "\n",
    "    def val_dist(self, datalist):\n",
    "        data = datalist[0]\n",
    "        if data.size % 365 != 0:\n",
    "            raise Exception('Data array length is not an integer multiple of 365')   \n",
    "        byyear = data.reshape(data.size // 365, 365)\n",
    "        \n",
    "        if self.want_max:\n",
    "            vals = np.sum(byyear >= self.var_threshold, axis=1)\n",
    "        else:\n",
    "            vals = np.sum(byyear <= self.var_threshold, axis=1)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "    \n",
    "class RangeDays(Hazard):\n",
    "    def __init__(self, hazname, varname, var_thresholdlow, var_thresholdhigh, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = varname\n",
    "        self.var_thresholdlow = var_thresholdlow\n",
    "        self.var_thresholdhigh = var_thresholdhigh\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "\n",
    "    def val_dist(self, datalist):\n",
    "        data = datalist[0]\n",
    "        byyear = data.reshape(data.size // 365, 365)\n",
    "        \n",
    "        vals = np.sum(np.logical_and(byyear >= self.var_thresholdlow, byyear <= self.var_thresholdhigh), axis=1)\n",
    "\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "        \n",
    "class Dryduration_seasonal_count(Hazard):\n",
    "    def __init__(self, hazname, startdate, enddate, southern_hem, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = 'pr'\n",
    "        self.startdate = startdate\n",
    "        self.enddate = enddate\n",
    "        self.southern_hem = southern_hem\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "        \n",
    "    def tf_array(self, datalist):\n",
    "        data = datalist[0] == 0\n",
    "        if data.size % 365 != 0:\n",
    "            raise Exception('Data array length is not an integer multiple of 365')   \n",
    "        byyear = data.reshape(data.size//365, 365)\n",
    "        \n",
    "        start_jday = d2j('1999-{0}'.format(self.startdate)) - [0, 182][int(self.southern_hem)]\n",
    "        end_jday = d2j('1999-{0}'.format(self.enddate)) - [0, 182][int(self.southern_hem)]\n",
    "        if end_jday < start_jday:\n",
    "            end_jday += 365\n",
    "        inseason_onerow = [((i >= start_jday)and(i <= end_jday)) for i in range(365)]\n",
    "        inseason = np.array(inseason_onerow * (data.size//365))\n",
    "        inseason = inseason.reshape(data.size//365, 365)\n",
    "        return byyear * inseason\n",
    "\n",
    "    def val_dist(self, datalist): \n",
    "        byyear = self.tf_array(datalist)\n",
    "        vals = np.apply_along_axis(longest_run, 1, byyear)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "    \n",
    "class Drycount_seasonal(Hazard):\n",
    "    def __init__(self, hazname, startdate, enddate, southern_hem, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = 'pr'\n",
    "        self.startdate = startdate\n",
    "        self.enddate = enddate\n",
    "        self.southern_hem = southern_hem\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "        \n",
    "    def tf_array(self, datalist):\n",
    "        \n",
    "        data = datalist[0] == 0\n",
    "        if data.size % 365 != 0:\n",
    "            raise Exception('Data array length is not an integer multiple of 365')   \n",
    "        byyear = data.reshape(data.size//365, 365)\n",
    "        \n",
    "        start_jday = d2j('1999-{0}'.format(self.startdate)) - [0, 182][int(self.southern_hem)]\n",
    "        end_jday = d2j('1999-{0}'.format(self.enddate)) - [0, 182][int(self.southern_hem)]\n",
    "        if end_jday < start_jday:\n",
    "            end_jday += 365\n",
    "        inseason_onerow = [((i >= start_jday)and(i <= end_jday)) for i in range(365)]\n",
    "        inseason = np.array(inseason_onerow * data.size//365)\n",
    "        return (byyear * inseason) == 0\n",
    "\n",
    "    def val_dist(self, datalist):\n",
    "        tfarray = self.tf_array(datalist)\n",
    "        vals = np.apply_along_axis(count_runs, 1, tfarray, self.min_duration)\n",
    "        \n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "\n",
    "\n",
    "class Annual_val(Hazard):\n",
    "    def __init__(self, hazname, varname, aggtype, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = varname\n",
    "        self.aggtype = aggtype\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "        \n",
    "    def val_dist(self, datalist):\n",
    "        data = datalist[0]\n",
    "        byyear = data.reshape(data.size//365, 365)\n",
    "        if self.aggtype == 'sum':\n",
    "            vals = np.sum(byyear, axis=1)\n",
    "        elif self.aggtype == 'mean':\n",
    "            vals = np.mean(byyear, axis=1)\n",
    "        elif self.aggtype == 'max':\n",
    "            vals = np.max(byyear, axis=1)\n",
    "        elif self.aggtype == 'min':\n",
    "            vals = np.min(byyear, axis=1)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "\n",
    "class Seasonal_val(Hazard):\n",
    "    def __init__(self, hazname, varname, aggtype, startdate, enddate, southern_hem, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = varname\n",
    "        self.aggtype = aggtype\n",
    "        self.startdate = startdate\n",
    "        self.enddate = enddate\n",
    "        self.southern_hem = southern_hem\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "        \n",
    "    def val_dist(self, datalist):\n",
    "        data = datalist[0]\n",
    "        byyear = data.reshape(data.size//365, 365)\n",
    "        start_jday = d2j('1999-{0}'.format(self.startdate)) - [0, 182][int(self.southern_hem)]\n",
    "        end_jday = d2j('1999-{0}'.format(self.enddate)) - [0, 182][int(self.southern_hem)]\n",
    "        if end_jday < start_jday:\n",
    "            end_jday += 365\n",
    "        inseason_onerow = [((i >= start_jday)and(i <= end_jday)) for i in range(365)]\n",
    "        inseason = np.array([inseason_onerow]*(data.size//365))\n",
    "        byyear = byyear * inseason\n",
    "        if self.aggtype == 'sum':\n",
    "            vals = np.sum(byyear, axis=1)\n",
    "        elif self.aggtype == 'mean':\n",
    "            vals = np.mean(byyear, axis=1)\n",
    "        elif self.aggtype == 'max':\n",
    "            vals = np.max(byyear, axis=1)\n",
    "        elif self.aggtype == 'min':\n",
    "            vals = np.min(byyear, axis=1)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        print(vals)\n",
    "        return result_dist\n",
    "\n",
    "class ARIDays(Hazard):\n",
    "    def __init__(self, hazname, ari95, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = 'pr'\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "\n",
    "    def val_dist(self, datalist):\n",
    "        data = datalist[0]\n",
    "        if data.size % 365 != 0:\n",
    "            raise Exception('Data array length is not an integer multiple of 365')   \n",
    "        byyear = data.reshape(data.size // 365, 365)\n",
    "        \n",
    "        aris = np.apply_along_axis(ari, 1, byyear)\n",
    "        vals = np.sum(aris >= ari95, axis=1)\n",
    "        \n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "    \n",
    "class WetbulbDays(Hazard):\n",
    "    def __init__(self, hazname, wbt_threshold, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = 'maxwetbulb'\n",
    "        self.wbt_threshold = wbt_threshold\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "    \n",
    "    def val_dist(self, datalist):\n",
    "        data_t = datalist[0]\n",
    "        data_h = datalist[1]\n",
    "        data = wetbulbtemp(data_t, data_h)\n",
    "        if data.size % 365 != 0:\n",
    "            raise Exception('Data array length is not an integer multiple of 365')\n",
    "        byyear = data.reshape(data.size//365, 365)\n",
    "        vals = np.sum(byyear >= self.wbt_threshold, axis=1)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "\n",
    "class DroughtSPIDays(Hazard):\n",
    "    def __init__(self, hazname, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = 'pr'\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "    \n",
    "    def val_dist(self, datalist):\n",
    "        data = datalist[0]\n",
    "        if data.size % 365 != 0:\n",
    "            raise Exception('Data array length is not an integer multiple of 365')\n",
    "        \n",
    "        t=pd.date_range(start='1980-01-01', end='{0}-12-31'.format(1980 + (data.size//365) - 1), freq='D')\n",
    "        t = t[~((t.month == 2) & (t.day == 29))]\n",
    "        \n",
    "        try:\n",
    "            droughtdays = spei.spi(pd.Series(data, index=t)).to_numpy()\n",
    "        except:\n",
    "            return None\n",
    "        byyear = droughtdays.reshape(data.size // 365, 365)\n",
    "        \n",
    "        vals = np.sum(byyear <= -1, axis=1)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "    \n",
    "class DegreeDays(Hazard):\n",
    "    def __init__(self, hazname, reftemp, want_max, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = 'tas'\n",
    "        self.reftemp = reftemp\n",
    "        self.extremeval = extremeval\n",
    "        self.want_max = want_max\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "\n",
    "    def val_dist(self, datalist):\n",
    "        data = datalist[0]\n",
    "        if data.size % 365 != 0:\n",
    "            raise Exception('Data array length is not an integer multiple of 365')   \n",
    "        byyear = data.reshape(data.size // 365, 365)\n",
    "        \n",
    "        if self.want_max:\n",
    "            vals = np.sum(np.round(np.maximum(0, byyear - self.reftemp)), axis=1)\n",
    "        else:\n",
    "            vals = np.sum(np.round(np.maximum(0, (-1 * byyear) + self.reftemp)), axis=1)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b21c38fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_observed_gee(varname, latlon, start_year, end_year, southern_hem=False):\n",
    "    def relhum(T, Tdp):\n",
    "        T = T.astype('float64')\n",
    "        Tdp = Tdp.astype('float64')\n",
    "        numerator = np.exp(17.625 * Tdp / (243.04 + Tdp))\n",
    "        denominator = np.exp(17.625 * T / (243.04 + T))\n",
    "        return 100 * numerator / denominator\n",
    "\n",
    "    def get_eradata(varname, southern_hem=False):\n",
    "        # Return numpy array in correct units, leapdays removed\n",
    "        dataset = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\n",
    "        gee_geom = ee.Geometry.Point((latlon[1], latlon[0]))\n",
    "        data_vars = dataset.select(varname).filter(ee.Filter.date('{0}-01-01'.format(start_year), '{0}-01-01'.format(end_year+ 1)))\n",
    "        success = False\n",
    "        while not success:\n",
    "            try:\n",
    "                d = data_vars.getRegion(gee_geom, 2500, 'epsg:4326').getInfo()\n",
    "                success = True\n",
    "            except:\n",
    "                print('\\nRetrying')\n",
    "        result = [i[4] for i in d[1:]]\n",
    "        return np.array(result)\n",
    "    \n",
    "    if varname == 'hurs':\n",
    "        success = False\n",
    "        era_dewpoint = get_eradata('dewpoint_2m_temperature')-273.15\n",
    "        era_maxtemp = get_eradata('maximum_2m_air_temperature')-273.15\n",
    "        hist_obs = relhum(era_maxtemp, era_dewpoint)\n",
    "    elif varname == 'pr':\n",
    "        hist_obs = get_eradata('total_precipitation') * 1000\n",
    "    elif varname == 'tasmax':\n",
    "        hist_obs = get_eradata('maximum_2m_air_temperature')-273.15\n",
    "    else:    # varname == 'tasmin'\n",
    "        hist_obs = get_eradata('minimum_2m_air_temperature')-273.15\n",
    "    return removeLeapDays(hist_obs, start_year, end_year, southern_hem)\n",
    "\n",
    "def get_modeled_gee(varname, scenario, model, lat, lon, southern_hem, start_year, end_year):\n",
    "    # Return numpy array in correct units, leapdays removed\n",
    "    dataset = ee.ImageCollection('NASA/GDDP-CMIP6').filter(ee.Filter.eq('model', model)).filter(ee.Filter.eq('scenario', scenario))\n",
    "    gee_geom = ee.Geometry.Point((lon, lat))\n",
    "    if start_year >= 2015:\n",
    "        if southern_hem:\n",
    "            data_vars = dataset.select(varname).filter(ee.Filter.date('{0}-07-01'.format(start_year-1), '{0}-07-01'.format(end_year)))\n",
    "        else:\n",
    "            data_vars = dataset.select(varname).filter(ee.Filter.date('{0}-01-01'.format(start_year), '{0}-01-01'.format(end_year+ 1)))\n",
    "        result = [i[4] for i in data_vars.getRegion(gee_geom, 2500, 'epsg:4326').getInfo()[1:]]\n",
    "    else:\n",
    "        hist_dataset = ee.ImageCollection('NASA/GDDP-CMIP6').filter(ee.Filter.eq('model', model))\n",
    "        if southern_hem:\n",
    "            hist_part = hist_dataset.select(varname).filter(ee.Filter.eq('scenario', 'historical')).filter(ee.Filter.date('{0}-07-01'.format(start_year-1), '2015-01-01'))\n",
    "            if end_year >= 2015:\n",
    "                ssp_part = dataset.select(varname).filter(ee.Filter.eq('scenario', scenario)).filter(ee.Filter.date('2015-01-01', '{0}-07-01'.format(end_year)))\n",
    "        else:\n",
    "            hist_part = hist_dataset.select(varname).filter(ee.Filter.eq('scenario', 'historical')).filter(ee.Filter.date('{0}-01-01'.format(start_year), '2015-01-01'))\n",
    "            if end_year >= 2015:\n",
    "                ssp_part = dataset.select(varname).filter(ee.Filter.eq('scenario', scenario)).filter(ee.Filter.date('2015-01-01'.format(start_year-1), '{0}-01-01'.format(end_year+ 1)))\n",
    "        hist_result = [i[4] for i in hist_part.getRegion(gee_geom, 2500, 'epsg:4326').getInfo()[1:]]\n",
    "        if end_year >= 2015:\n",
    "            ssp_result = [i[4] for i in ssp_part.getRegion(gee_geom, 2500, 'epsg:4326').getInfo()[1:]]\n",
    "        else:\n",
    "            ssp_result = []\n",
    "        result = hist_result + ssp_result\n",
    "    d =  VARIABLES[varname]['nex_transform'](np.array(result))\n",
    "    return removeLeapDays(d, start_year, end_year, southern_hem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9066e5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdata(varnames, warmingscenario, lat, lon, best_models):\n",
    "    res = {varname: {} for varname in varnames}\n",
    "    for varname in varnames:\n",
    "        for model in best_models[varname]:\n",
    "            scenario = scenarioyears[model][warmingscenario][0]\n",
    "            start_year = scenarioyears[model][warmingscenario][1] - 4\n",
    "            end_year = scenarioyears[model][warmingscenario][1] + 5\n",
    "            res[varname][model] = (start_year, end_year, get_modeled_gee(varname, scenario, model, lat, lon, lat < 0, start_year, end_year))\n",
    "    return res\n",
    "\n",
    "def do_locationhazard(loc_id, hazard, datasets, latlon, scenario, calib_fxns):\n",
    "    lat, lon = latlon    \n",
    "    return loc_id, lat, lon, hazard.hazname, scenario, hazard.get_expectedval(latlon, datasets, calib_fxns), hazard.get_exceedanceprob(latlon, datasets, calib_fxns)\n",
    "    #return hazard.get_exceedanceprob(latlon, datasets, calib_fxns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f591121",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYNTHVARS = {\n",
    "    'maxwetbulb': {\n",
    "        'nex_varnames': ['tasmax', 'hurs'],\n",
    "        'era_varnames': ['maximum_2m_air_temperature', 'dewpoint_2m_temperature'],\n",
    "        'nex_fxn': lambda a, b: wetbulbtemp(a,b),\n",
    "        'era_fxn': lambda a, b: wetbulbtemp(a, relhum(a, b)),\n",
    "        'era_transform': [lambda x: x - 273.15, lambda x: x - 273.15]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66fef401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calibfxns(varname, cityinfo):\n",
    "    cf = {loc_id: {} for loc_id in range(len(cityinfo))}\n",
    "    for varname in varname.split('+'):\n",
    "        with open('bmcf_{0}.txt'.format(varname), 'r') as ifile:\n",
    "            lines = ifile.readlines()\n",
    "            for linenum, line in enumerate(lines):\n",
    "                items = [i.strip() for i in line.split('\\t')]\n",
    "                if linenum % 3 == 0:\n",
    "                    cf[int(items[0])][varname] = {}\n",
    "                cf[int(items[0])][varname][items[1]] = json.loads(items[2])\n",
    "    return cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9af2291",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarioyears = {}\n",
    "with open('scenarioyears.csv', 'r') as ifile:\n",
    "    lines = ifile.readlines()\n",
    "    for line in lines:\n",
    "        items = [i.strip() for i in line.split(',')]\n",
    "        scenarioyears[items[0]] = {\n",
    "            'baseline': ('ssp245', 2015),\n",
    "            '1.5C': (items[1], int(items[2])),\n",
    "            '2.0C': (items[1], int(items[3])),\n",
    "            '3.0C': (items[1], int(items[4]))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4638c514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tas\n",
      "600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 \n",
      "Retrying\n",
      "632 633 634 635 636 637 638 639 640 641 642 643 \n",
      "Retrying\n",
      "644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 \n",
      "Retrying\n",
      "760 761 \n",
      "Retrying\n",
      "\n",
      "Retrying\n",
      "762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 \n",
      "Retrying\n",
      "792 793 794 795 796 797 798 799 \n",
      "tasmax\n",
      "600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 CPU times: total: 36min 8s\n",
      "Wall time: 8h 2min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = []\n",
    "for varname in ['tas', 'tasmax']:#, 'tasmax', 'pr', 'maxwetbulb']:\n",
    "    print()\n",
    "    print(varname)\n",
    "    calibfxns = get_calibfxns(varname, CITYLATLON)\n",
    "    for cityname in list(CITYLATLON.keys())[600:800]:\n",
    "        lat, lon, loc_id = CITYLATLON[cityname]\n",
    "        loc_id = int(loc_id)\n",
    "        print(loc_id, end=' ')\n",
    "        latlon = (lat, lon)\n",
    "        tasmax90 = yearextreme_percentile('tasmax', 90, latlon, True)\n",
    "        pr90 = yearextreme_percentile('pr', 90, latlon, True)\n",
    "        ari95 = get_ari95(latlon)\n",
    "        HAZARDS = [\n",
    "            #Tempwave_count('Heatwavecount tmax 90pctl 3plus', 'tasmax', 3, tasmax90, True, [1, 3, 5]),\n",
    "            #Tempwave_duration('Heatwaveduration tmax 90pctl', 'tasmax', tasmax90, True, [20, 30, 40]),\n",
    "            \n",
    "            ThresholdDays('Days warmer than 35', 'tasmax', 35, True, [10, 20, 30]),\n",
    "            DegreeDays('CDD21', 21, True, [2000, 3000, 4000]),\n",
    "            #ThresholdDays('Days warmer than 40', 'tasmax', 40, True, [10, 20, 30]),\n",
    "            #ThresholdDays('Days warmer than than 95th pctle yearlong', 'tasmax', tasmax90, True, [60, 70, 80]),\n",
    "\n",
    "            #RangeDays('arbovirus temp', 'tas', 26, 29, [30, 60, 90]), #https://journals.plos.org/plosntds/article?id=10.1371/journal.pntd.0005568\n",
    "            #RangeDays('malaria temp', 'tas', 22.9, 27.8, [30, 60, 90]), # https://malariajournal.biomedcentral.com/articles/10.1186/s12936-020-03224-6\n",
    "\n",
    "            #Annual_val('Hottest annual temp', 'tasmax', 'max', [35, 40, 45]),\n",
    "            #Annual_val('Highest daily precip', 'pr', 'max', [500, 1000, 2000]),\n",
    "            #ThresholdDays('Days precip gte 90th pctl', 'pr', pr90, True, [20, 30, 40]),\n",
    "            #ARIDays('ARI days gte ari95', ari95, [5, 10, 20]),\n",
    "            #WetbulbDays('Wetbulb days gte 31', 31, [10, 25, 30]),\n",
    "            #DroughtSPIDays('Drought days SPI lte -1', [100, 140, 180])\n",
    "        ]\n",
    "        warming_scenarios = ['baseline', '1.5C', '2.0C', '3.0C']\n",
    "        varnames = []\n",
    "        for hazard in HAZARDS:\n",
    "            if hazard.varname == varname:\n",
    "                for scenario in warming_scenarios:\n",
    "                    if varname in SYNTHVARS:\n",
    "                        varnames += SYNTHVARS[varname]['nex_varnames']\n",
    "                    else:\n",
    "                        varnames += hazard.varname.split('+')\n",
    "        varnames = list(set(varnames))\n",
    "        allds = {scenario: getdata(varnames, scenario, lat, lon, {vn: list(calibfxns[loc_id][varname].keys()) for vn in varnames})\n",
    "                    for scenario in warming_scenarios}\n",
    "        for hazard in HAZARDS:\n",
    "            if hazard.varname == varname:\n",
    "                for scenario in warming_scenarios:\n",
    "                    if varname in SYNTHVARS:\n",
    "                        varnames = SYNTHVARS[varname]['nex_varnames']\n",
    "                    else:\n",
    "                        varnames = hazard.varname.split('+')\n",
    "                    datasets = allds[scenario]\n",
    "                    res = do_locationhazard(loc_id, hazard, datasets, (lat, lon), scenario, calibfxns[loc_id])\n",
    "                    rlocid, rlat, rlon, rhazname, rscenario, rev, rprob = res\n",
    "                    with open('wp_results_d1.csv', 'a') as ifile:\n",
    "                        for rank, rmod in enumerate(list(rev.keys())):\n",
    "                            if len(rmod.split('+')) > 1:\n",
    "                                displaymod = rmod.split('+')[0]\n",
    "                            else:\n",
    "                                displaymod = rmod\n",
    "                            ifile.write('{0},{1},{2},{3},{4},'.format(rlocid, rlat, rlon, rhazname, rscenario))\n",
    "                            ifile.write('{0},{1},{2},{3},{4}'.format(displaymod, rank + 1, rev[rmod][0], rev[rmod][1], rev[rmod][2]))\n",
    "                            for rexval in rprob[rmod]:\n",
    "                                ifile.write(',{0},{1},{2},{3}'.format(rexval, rprob[rmod][rexval][0], rprob[rmod][rexval][1], rprob[rmod][rexval][2]))\n",
    "                            ifile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67fce45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (halfdegenv)",
   "language": "python",
   "name": "halfdegenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
